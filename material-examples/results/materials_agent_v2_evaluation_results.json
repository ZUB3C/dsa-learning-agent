{
  "test_suite": "materials_agent_v2_evaluation",
  "base_url": "http://localhost:8000",
  "overall_metrics": {
    "total_tests": 15,
    "successful_tests": 15,
    "failed_tests": 0,
    "success_rate": 100.0,
    "avg_completeness": 0.627,
    "avg_relevance": 0.793,
    "avg_quality": 0.707,
    "min_completeness": 0.4,
    "max_completeness": 0.75,
    "min_relevance": 0.5,
    "max_relevance": 1.0,
    "min_quality": 0.5,
    "max_quality": 0.8,
    "avg_generation_time": 153.93,
    "total_generation_time": 2308.88,
    "avg_tot_iterations": 18.0,
    "avg_explored_nodes": 18.0,
    "avg_documents_collected": 7.4,
    "total_cost_usd": 0.52,
    "avg_cost_per_test": 0.0347,
    "total_gigachat2_calls": 130,
    "total_gigachat3_calls": 520,
    "breakdown_by_level": {
      "beginner": {
        "count": 5,
        "avg_completeness": 0.69,
        "avg_relevance": 0.8400000000000001,
        "avg_quality": 0.78,
        "avg_generation_time": 152.1653841495514,
        "avg_cost": 0.036
      },
      "intermediate": {
        "count": 5,
        "avg_completeness": 0.5900000000000001,
        "avg_relevance": 0.74,
        "avg_quality": 0.6399999999999999,
        "avg_generation_time": 157.2823205947876,
        "avg_cost": 0.0344
      },
      "advanced": {
        "count": 5,
        "avg_completeness": 0.6,
        "avg_relevance": 0.8,
        "avg_quality": 0.7,
        "avg_generation_time": 152.32795939445495,
        "avg_cost": 0.0336
      }
    }
  },
  "individual_results": [
    {
      "topic_id": "beginner_01",
      "topic": "Линейный поиск",
      "user_level": "beginner",
      "success": true,
      "material_length": 4533,
      "word_count": 637,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.75,
      "documents_collected": 5,
      "completeness_score": 0.75,
      "relevance_score": 0.8,
      "quality_score": 0.8,
      "generation_time_seconds": 129.17974305152893,
      "gigachat2_calls": 9,
      "gigachat3_calls": 36,
      "estimated_cost_usd": 0.036,
      "tools_used": [
        "extract_concepts",
        "corrective_check",
        "adaptive_rag_search"
      ],
      "generation_id": "gen_32ceecebf368",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Линейный поиск — подробное руководство для начинающих\n\n## Введение\n\nПредставьте ситуацию: вы ищете книгу на полке, где книги расположены хаотично. Вы просматриваете каждую книгу одну за другой до тех пор, пока не найдёте нужную. Этот подход называется линейным поиском. \n\nЛинейный поиск — это базовый метод нахождения элемента в списке или массиве. Его используют тогда, когда другие методы (например, бинарный поиск) неприменимы либо неэффективны. Несмотря на свою простоту, этот алгоритм лежит в основе многих более продвинутых техник обработки данных.\n\n## Основная теория\n\n### Что такое линейный поиск?\n\nЛинейный поиск (или последовательный поиск) — это способ найти нужный элемент в коллекции данных путём проверки каждого элемента по очереди, начиная с первого. Алгоритм останавливается, когда находит искомый объект или доходит до конца списка.\n\n#### Пример:\nДопустим, у нас есть список чисел `[132, 612, 232, 890, 161, 222, 123, 861, 120, 330]`, и нам нужно найти число `222`. Мы последовательно проверяем каждый элемент, пока не найдем нужное значение.\n\n```\n| Индекс | Значение |\n|--------|----------|\n| 0      | 132      |\n| 1      | 612      |\n| 2      | 232      |\n| 3      | 890      |\n| 4      | 161      |\n| 5      | 222      |\n| 6      | 123      |\n| 7      | 861      |\n| 8      | 120      |\n| 9      | 330      |\n```\n\nМы начинаем проверку с начала списка и продолжаем двигаться дальше, пока не достигнем нужного числа (`222` находится на пятой позиции).\n\n### Принцип работы\n\nАлгоритм работает следующим образом:\n\n1. Начинаем с первого элемента списка.\n2. Проверяем, совпадает ли текущее значение с искомым.\n3. Если да — возвращаем индекс найденного элемента.\n4. Если нет — двигаемся к следующему элементу.\n5. Повторяем шаги 2–4, пока не пройдём весь список.\n\n### Временная и пространственная сложность\n\n- **Время выполнения**: O(n), где n — количество элементов в списке. Это значит, что скорость поиска зависит от размера списка: чем больше элементов, тем дольше будет выполняться поиск.\n  \n- **Пространственная сложность**: O(1), поскольку дополнительные структуры данных не требуются.\n\n### Особенности\n\n- Работает даже на несортированных списках.\n- Не требует предварительной подготовки данных.\n- Подходит для небольших наборов данных или ситуаций, когда сортировка невозможна/нецелесообразна.\n\n## Практический пример\n\nРассмотрим реализацию линейного поиска на Python:\n\n```python\ndef linear_search(arr, target):\n    # Проходим по каждому элементу массива\n    for index in range(len(arr)):\n        if arr[index] == target:\n            return index  # Возвращаем позицию, если нашли элемент\n    return None  # Если ничего не найдено\n\n# Тестируем функцию\nnumbers = [132, 612, 232, 890, 161, 222, 123, 861, 120, 330]\nresult = linear_search(numbers, 222)\nprint(\"Позиция:\", result)  # Выведет: Позиция: 5\n```\n\n### Шаги пошагово:\n\n1. Создаётся функция `linear_search()`.\n2. Внутри функции проходит цикл по каждому элементу массива.\n3. Каждый элемент сравнивается с целевым значением.\n4. Если совпадение найдено, возвращается позиция элемента.\n5. Если после полного прохода совпадение не обнаружено, возвращается `None`.\n\n## Сравнение с аналогами\n\n### Альтернативы:\n\n- **Бинарный поиск**: гораздо эффективнее, но требует предварительно отсортированного массива.\n- **Хеширование**: позволяет искать почти мгновенно, но требует дополнительной памяти и подходит не во всех случаях.\n\n### Преимущества линейного поиска:\n\n- Очень простая реализация.\n- Нет требований к порядку элементов.\n- Минимальные затраты ресурсов.\n\n### Недостатки:\n\n- Медленный на больших наборах данных.\n- Эффективность снижается пропорционально количеству элементов.\n\n## Практические советы\n\n1. Используйте линейный поиск, если ваш список небольшой или заранее неизвестно, какой именно элемент потребуется.\n   \n2. Для ускорения поиска попробуйте хранить наиболее востребованные элементы ближе к началу списка.\n\n3. Если вам постоянно приходится искать одни и те же элементы, подумайте о создании хэш-таблицы или другого быстрого способа хранения данных.\n\n4. Всегда учитывайте размер вашего набора данных перед выбором метода поиска.\n\n5. Помните, что иногда лучше потратить немного времени на предварительную обработку (сортировку), чтобы потом получить выигрыш в скорости поиска.\n\n## Заключение\n\nЛинейный поиск — это фундаментальная техника, которую должен освоить любой программист. Хотя она проста, её понимание поможет заложить основу для освоения более сложных методов поиска и оптимизации данных.\n\nПродолжайте изучать алгоритмы и структуры данных!"
    },
    {
      "topic_id": "beginner_02",
      "topic": "Стек как структура данных",
      "user_level": "beginner",
      "success": true,
      "material_length": 6148,
      "word_count": 701,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.7,
      "documents_collected": 7,
      "completeness_score": 0.7,
      "relevance_score": 0.8,
      "quality_score": 0.7,
      "generation_time_seconds": 158.60757207870483,
      "gigachat2_calls": 9,
      "gigachat3_calls": 36,
      "estimated_cost_usd": 0.036,
      "tools_used": [
        "web_search",
        "adaptive_rag_search",
        "corrective_check"
      ],
      "generation_id": "gen_f80c360e5a9f",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Учебный материал по теме: Стек как структура данных  \n\n## Введение  \n\nСтек — одна из базовых структур данных, используемых практически повсеместно в разработке программного обеспечения. Его принцип работы интуитивно понятен каждому человеку благодаря повседневной жизни: вспомните стопку книг или тарелок — последнюю положенную книгу или тарелку вы снимаете первой. Этот же подход лежит в основе стека в информатике.  \n\nПочему изучение стека важно? Потому что эта простая концепция помогает эффективно решать широкий спектр задач программирования, начиная от обработки арифметических выражений и заканчивая управлением памятью и обработкой ошибок. Понимание принципов работы стека позволит вам писать более эффективные и надежные программы.  \n\n## Основная теория  \n\n### Определение стека  \n\nСтек (англ. *stack*) — это структура данных, организованная по принципу **«последним пришел — первым ушел»**, сокращенно обозначаемая как **LIFO (*Last In First Out*)**. Представьте себе стопку монет: монета, положенная сверху, снимается первой. Аналогичным образом работают данные в стеке: последние добавленные элементы извлекаются первыми.  \n\n### Основные операции над стеком  \n\nРассмотрим ключевые операции, поддерживаемые стеком:  \n\n- `push(item)` — добавляет элемент в верхушку стека.  \n- `pop()` — удаляет верхний элемент из стека и возвращает его значение.  \n- `peek()` — возвращает значение верхнего элемента, не удаляя его.  \n- `is_empty()` — проверяет, пустой ли стек.  \n\nЭти операции имеют временную сложность $\\mathcal{O}(1)$, то есть выполняются за константное время независимо от размера стека.  \n\n### Реализация стека  \n\nСуществует несколько способов реализации стека:  \n\n#### 1. На основе массива  \nЭто наиболее простое решение, когда элементы хранятся последовательно в одном массиве. Однако здесь возникает проблема перерастания массива, что усложняет управление памятью.  \n\n```python\nclass StackArray:\n    def __init__(self):\n        self.stack = []\n    \n    def push(self, item):\n        \"\"\" Добавляет элемент в стек \"\"\"\n        self.stack.append(item)\n    \n    def pop(self):\n        \"\"\" Удаляет и возвращает верхний элемент \"\"\"\n        if not self.is_empty():\n            return self.stack.pop()\n        else:\n            raise IndexError('Stack is empty')\n    \n    def peek(self):\n        \"\"\" Возвращает верхний элемент без удаления \"\"\"\n        if not self.is_empty():\n            return self.stack[-1]\n        else:\n            raise IndexError('Stack is empty')\n    \n    def is_empty(self):\n        \"\"\" Проверяет, пуст ли стек \"\"\"\n        return len(self.stack) == 0\n```\n\n#### 2. На основе связанного списка  \nЭтот способ предпочтительнее, поскольку связанному списку не нужна предварительная память под весь объем данных. Каждый элемент содержит ссылку на следующий элемент, образуя цепочку.  \n\n```python\nclass Node:\n    def __init__(self, value=None):\n        self.value = value\n        self.next = None\n\nclass StackLinkedList:\n    def __init__(self):\n        self.top = None\n    \n    def push(self, item):\n        \"\"\" Добавляет элемент в стек \"\"\"\n        new_node = Node(item)\n        new_node.next = self.top\n        self.top = new_node\n    \n    def pop(self):\n        \"\"\" Удаляет и возвращает верхний элемент \"\"\"\n        if not self.is_empty():\n            popped_value = self.top.value\n            self.top = self.top.next\n            return popped_value\n        else:\n            raise IndexError('Stack is empty')\n    \n    def peek(self):\n        \"\"\" Возвращает верхний элемент без удаления \"\"\"\n        if not self.is_empty():\n            return self.top.value\n        else:\n            raise IndexError('Stack is empty')\n    \n    def is_empty(self):\n        \"\"\" Проверяет, пуст ли стек \"\"\"\n        return self.top is None\n```\n\n### Применение стека  \n\nСтеки используются в различных областях программирования:  \n\n- **Обработка выражений**: проверка правильности расстановки скобок, вычисление постфиксных выражений.  \n- **Алгоритмы обхода графа**: глубина первого поиска (DFS).  \n- **Реализовать историю отмены/повтора**: редакторы текста используют стек для хранения предыдущих состояний документа.  \n- **Управление памятью**: компиляторы и интерпретаторы используют стек для вызова функций и управления локальными переменными.  \n\n## Примеры  \n\n### Пример №1: Разворот строки  \nЗадача: развернуть строку с помощью стека.  \n\n```python\ndef reverse_string(s):\n    stack = []\n    for char in s:\n        stack.append(char)\n    reversed_string = ''\n    while stack:\n        reversed_string += stack.pop()\n    return reversed_string\n\nprint(reverse_string(\"hello\"))  # Выведет 'olleh'\n```\n\n### Пример №2: Проверка сбалансированности скобок  \nЗадача: определить, правильно ли расставлены скобки в строке.  \n\n```python\ndef check_brackets(expression):\n    brackets_map = {'(': ')', '{': '}', '[': ']'}\n    open_brackets = set(brackets_map.keys())\n    close_brackets = set(brackets_map.values())\n    stack = []\n    \n    for char in expression:\n        if char in open_brackets:\n            stack.append(char)\n        elif char in close_brackets:\n            if not stack or brackets_map[stack.pop()] != char:\n                return False\n    return len(stack) == 0\n\nprint(check_brackets(\"{()}[]\"))  # True\nprint(check_brackets(\"[({)]\"))   # False\n```\n\n## Практические советы  \n\n- Всегда проверяйте стек на пустоту перед операциями извлечения (`pop`, `peek`), иначе возможны исключения.  \n- Используйте стек там, где важна последовательность выполнения операций (например, обработка вложенных конструкций).  \n- Помните про ограничения производительности разных реализаций стека (массивы vs списки): выбирайте подходящий вариант исходя из требований вашей задачи.  \n- Не забывайте очищать стек после завершения работы, особенно если речь идет о больших объемах данных.  \n\n## Дополнительные ресурсы  \n\n- [Документация Python](https://docs.python.org/3/tutorial/datastructures.html#using-lists-as-stacks)  \n- Книга: Томас Кормен, Чарльз Лейзерсон, Рональд Ривест, Клиффорд Штайн. Алгоритмы: построение и анализ. Глава 10.  \n\nНадеюсь, этот материал поможет вам лучше понять работу стека и применять эту структуру данных в ваших проектах!"
    },
    {
      "topic_id": "beginner_03",
      "topic": "Очередь как структура данных",
      "user_level": "beginner",
      "success": true,
      "material_length": 4842,
      "word_count": 533,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.7,
      "documents_collected": 11,
      "completeness_score": 0.7,
      "relevance_score": 0.8,
      "quality_score": 0.8,
      "generation_time_seconds": 162.0235733985901,
      "gigachat2_calls": 9,
      "gigachat3_calls": 36,
      "estimated_cost_usd": 0.036,
      "tools_used": [
        "adaptive_rag_search",
        "web_search",
        "corrective_check"
      ],
      "generation_id": "gen_a6c50ac9f34e",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Очередь как структура данных  \n\n## Введение  \n\nОчередь — это простая, но важная структура данных, которая позволяет организовывать обработку элементов последовательно, следуя правилу \"первый пришёл — первый ушёл\" (FIFO, first-in-first-out). Она похожа на обычную жизненную очередь, где новый человек становится последним, а обслуживание начинается с самого первого посетителя. \n\nОчереди используются повсеместно: от управления потоками запросов в операционных системах до организации задач в приложениях реального времени. Их простота делает их незаменимым инструментом для многих областей программирования.\n\n## Основная теория  \n\n### Определение  \n\nОчередь — это упорядоченная коллекция элементов, где добавление новых элементов происходит в конец коллекции (`enqueue`), а удаление — из начала (`dequeue`).\n\n### Основные операции  \n\n- `enqueue(element)` — добавляет элемент в конец очереди.  \n- `dequeue()` — извлекает и удаляет первый элемент из очереди.  \n- `peek()` — показывает первый элемент, не удаляя его.  \n- `is_empty()` — проверяет, пуста ли очередь.  \n- `size()` — возвращает количество элементов в очереди.  \n\n### Реализации  \n\n#### 1. Очередь на основе связанного списка  \nЭто наиболее распространённый способ реализации очереди. Каждый элемент представлен узлом, содержащим данные и ссылку на следующий узел. Первый узел называется головным (`front`), последний — хвостовым (`rear`).  \n\n```python\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\nclass QueueLinkedList:\n    def __init__(self):\n        self.front = None\n        self.rear = None\n    \n    def enqueue(self, item):\n        \"\"\"Добавляет элемент в конец очереди.\"\"\"\n        node = Node(item)\n        if self.rear is None:\n            self.front = self.rear = node\n        else:\n            self.rear.next = node\n            self.rear = node\n            \n    def dequeue(self):\n        \"\"\"Удаляет элемент из начала очереди.\"\"\"\n        if self.is_empty():\n            raise Exception(\"Очередь пуста\")\n        temp = self.front\n        self.front = temp.next\n        \n        if self.front is None:\n            self.rear = None\n        return temp.data\n    \n    def peek(self):\n        \"\"\"Возвращает первый элемент очереди.\"\"\"\n        if self.is_empty():\n            raise Exception(\"Очередь пуста\")\n        return self.front.data\n    \n    def is_empty(self):\n        \"\"\"Проверяет, пуста ли очередь.\"\"\"\n        return self.front is None\n```\n\n#### 2. Очередь на основе двух стеков  \nМожно реализовать очередь, используя два стека. Один стек служит для добавления элементов, другой — для их извлечения.  \n\n```python\nclass QueueWithStacks:\n    def __init__(self):\n        self.input_stack = []\n        self.output_stack = []\n    \n    def enqueue(self, item):\n        \"\"\"Добавляет элемент в очередь.\"\"\"\n        self.input_stack.append(item)\n    \n    def dequeue(self):\n        \"\"\"Удаляет элемент из очереди.\"\"\"\n        if not self.output_stack:\n            while self.input_stack:\n                self.output_stack.append(self.input_stack.pop())\n        if not self.output_stack:\n            raise Exception(\"Очередь пуста\")\n        return self.output_stack.pop()\n    \n    def peek(self):\n        \"\"\"Возвращает первый элемент очереди.\"\"\"\n        if not self.output_stack:\n            while self.input_stack:\n                self.output_stack.append(self.input_stack.pop())\n        if not self.output_stack:\n            raise Exception(\"Очередь пуста\")\n        return self.output_stack[-1]\n    \n    def is_empty(self):\n        \"\"\"Проверяет, пуста ли очередь.\"\"\"\n        return not (self.input_stack or self.output_stack)\n```\n\n### Временная и пространственная сложность  \n\n| Операция | Время |\n|----------|-------|\n| Enqueue  | O(1)  |\n| Dequeue  | O(n)   |\n| Peek     | O(1)  |\n| Is Empty | O(1)  |\n\nПространственная сложность обеих реализаций — O(n), где n — количество элементов в очереди.\n\n## Практическое применение  \n\nОчереди часто встречаются в следующих ситуациях:  \n\n- Обработка событий в операционной системе.  \n- Организация потоков задач в многозадачных программах.  \n- Управление буферизацией данных в сетевых протоколах.  \n- Планировщики задач и обработка сообщений.  \n\n## Советы и лучшие практики  \n\n1. Всегда проверяйте очередь на пустоту перед извлечением элементов.  \n2. Выбирайте подходящую реализацию исходя из требований производительности.  \n3. Не забывайте учитывать ограничения размера очереди, особенно при работе с большими объёмами данных.  \n4. Используйте встроенные классы очередей, предоставляемые языком программирования, если это возможно.  \n\n## Заключение  \n\nОчередь — это простая, но крайне полезная структура данных, позволяющая организовать последовательную обработку элементов. Её понимание и правильное использование существенно упрощают решение множества повседневных задач в разработке ПО."
    },
    {
      "topic_id": "beginner_04",
      "topic": "Связный список как структура данных",
      "user_level": "beginner",
      "success": true,
      "material_length": 5428,
      "word_count": 651,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.7,
      "documents_collected": 15,
      "completeness_score": 0.7,
      "relevance_score": 0.8,
      "quality_score": 0.8,
      "generation_time_seconds": 148.23655891418457,
      "gigachat2_calls": 9,
      "gigachat3_calls": 36,
      "estimated_cost_usd": 0.036,
      "tools_used": [
        "extract_concepts",
        "web_search",
        "adaptive_rag_search",
        "corrective_check"
      ],
      "generation_id": "gen_1f782ac4d964",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Связный список как структура данных  \n\n## Введение  \nСвязный список — одна из базовых структур данных, используемая для хранения последовательности элементов. Каждый элемент связанного списка содержит два компонента: данные и ссылку на следующий элемент. Это позволяет эффективно добавлять новые элементы в любое место списка без необходимости перемещать существующие элементы, как это происходит в массиве.  \n\nСвязные списки применяются там, где важна гибкость добавления и удаления элементов, особенно когда заранее неизвестно количество элементов или порядок их обработки. Например, такие структуры часто используются в операционных системах для управления памятью, в базах данных для организации индексов, а также в различных алгоритмах поиска и сортировки.  \n\n## Основная теория  \n\nСвязный список состоит из узлов (`node`), каждый узел хранит две части информации:  \n- `data` — сами данные узла (например, целое число, строка);  \n- `next` — ссылка на следующий узел в списке. Последний узел ссылается на специальный маркер конца списка (обычно обозначается как `null` или `None`).\n\nКлючевое преимущество связного списка заключается в возможности динамического изменения размера и порядка элементов без затрат памяти на перемещение существующих элементов. Однако доступ к элементу осуществляется последовательно, начиная с первого узла, что делает операцию случайного доступа менее эффективной по сравнению с массивом.  \n\nОсновные операции над связным списком включают:  \n- Добавление элемента в начало/конец/середину списка;  \n- Удаление элемента;  \n- Поиск элемента;  \n- Определение длины списка.  \n\nПространственная сложность связного списка составляет O(n), где n — количество элементов, поскольку каждому узлу соответствует отдельная область памяти. Время выполнения основных операций зависит от позиции элемента:  \n- Доступ к первому элементу — O(1);  \n- Обновление последнего элемента требует прохода по всему списку — O(n);  \n- Вставка/удаление элемента в середине списка — O(k), где k — позиция элемента.  \n\n## Пример реализации на Python  \nРассмотрим простую реализацию односвязного списка на Python:  \n```python\nclass Node:\n    def __init__(self, data=None):\n        self.data = data\n        self.next = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head = None\n\n    # Добавляем новый элемент в начало списка\n    def prepend(self, data):\n        new_node = Node(data)\n        new_node.next = self.head\n        self.head = new_node\n\n    # Добавляем новый элемент в конец списка\n    def append(self, data):\n        if not self.head:\n            self.head = Node(data)\n            return\n        current = self.head\n        while current.next:\n            current = current.next\n        current.next = Node(data)\n\n    # Печать содержимого списка\n    def display(self):\n        elements = []\n        current = self.head\n        while current:\n            elements.append(current.data)\n            current = current.next\n        print(elements)\n```\n\nПример использования:  \n```python\nllist = LinkedList()\nllist.prepend(\"Москва\")\nllist.append(\"Санкт-Петербург\")\nllist.display()  # ['Москва', 'Санкт-Петербург']\n```\n\n## Практический пример: реализация очереди FIFO  \nОчереди (First-In-First-Out) удобно реализовывать с использованием связных списков. Рассмотрим классическую реализацию очереди на основе связного списка:  \n```python\nclass Queue:\n    def __init__(self):\n        self.front = None\n        self.rear = None\n\n    def is_empty(self):\n        return self.front is None\n\n    def enqueue(self, item):  # Добавляем элемент в конец очереди\n        temp = Node(item)\n        if self.rear is None:\n            self.front = self.rear = temp\n            return\n        self.rear.next = temp\n        self.rear = temp\n\n    def dequeue(self):  # Убираем первый элемент из очереди\n        if self.is_empty():\n            return None\n        temp = self.front\n        self.front = temp.next\n        if self.front is None:\n            self.rear = None\n        return temp.data\n```\n\n## Сравнение с аналогами  \n\n| Характеристика | Массивы | Связные списки |\n|----------------|---------|-----------------|\n| Случайный доступ | Быстрый (O(1)) | Медленный (O(n)) |\n| Вставка/Удаление | Медленная (O(n)) | Быстрая (O(1)/O(k)) |\n| Размерность | Фиксированная | Динамическая |\n| Использование памяти | Эффективнее | Менее эффективно (из-за ссылок) |\n\nСвязные списки лучше подходят для ситуаций, когда важны частые вставки и удаления элементов, тогда как массивы предпочтительнее, если нужен быстрый доступ к конкретному индексу.  \n\n## Практические советы  \n\n- Всегда проверяйте, является ли список пустым, чтобы избежать ошибок обращения к несуществующим элементам.  \n- Используйте вспомогательные функции для упрощения операций (например, метод `display()` для вывода всех элементов).  \n- Не забывайте освобождать память, если используете языки программирования с ручным управлением памятью (C/C++).  \n- Старайтесь избегать циклических ссылок, иначе вы можете столкнуться с проблемами бесконечного цикла.  \n\n## Дополнительные ресурсы  \n- [Лекция 4](./лекция_4.pdf) — подробное описание принципов работы связных списков.  \n- [Алгоритмы и структуры данных](./алгоритмы_и_структуры_данных.pdf) — книга с глубокими теоретическими основами.  \n\nТаким образом, связный список — простая, но мощная структура данных, позволяющая эффективно решать широкий спектр задач, связанных с обработкой последовательностей элементов."
    },
    {
      "topic_id": "beginner_05",
      "topic": "Сложность алгоритмов Big O",
      "user_level": "beginner",
      "success": true,
      "material_length": 4803,
      "word_count": 615,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.6,
      "documents_collected": 6,
      "completeness_score": 0.6,
      "relevance_score": 1.0,
      "quality_score": 0.8,
      "generation_time_seconds": 162.77947330474854,
      "gigachat2_calls": 9,
      "gigachat3_calls": 36,
      "estimated_cost_usd": 0.036,
      "tools_used": [
        "web_search",
        "corrective_check",
        "adaptive_rag_search"
      ],
      "generation_id": "gen_9273f74ac8d4",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Учебный материал  \n## Сложность алгоритмов Big O  \n\n### Уровень подготовки: Beginner  \n\n#### Введение  \nКаждый день разработчики сталкиваются с необходимостью решать задачи, связанные с обработкой данных разного объёма. Чем больше объём данных, тем важнее становится понимать, насколько эффективно работает тот или иной алгоритм. Именно здесь вступает в игру концепция **сложности алгоритмов**, выраженная через нотацию **Big O**. Эта нотация позволяет сравнить разные алгоритмы и выбрать оптимальный вариант для конкретной ситуации. \n\nПредставьте себе ситуацию: вы работаете над проектом интернет-магазина, где пользователи ищут товары среди тысяч позиций. Важно, чтобы поиск был быстрым и удобным. Выбор правильного алгоритма поиска позволит сделать ваш магазин привлекательным для покупателей, увеличивая продажи и лояльность аудитории. Вот тут и пригодится понимание сложности алгоритмов.  \n\n#### Основная теория  \n\n##### Что такое Big O?  \n**Big O** — это специальный инструмент для описания скорости работы алгоритма относительно количества обрабатываемых данных. Проще говоря, она показывает, как изменится производительность программы, если увеличится объём данных.  \n\nНапример, если у вас есть список из 100 элементов, и вы хотите проверить наличие какого-либо элемента, самый простой способ — просмотреть весь список последовательно. Но если элементов станет миллион, такая проверка займёт значительно больше времени. Здесь и приходит на помощь Big O, помогая заранее предсказать, сколько ресурсов потребуется вашему алгоритму.  \n\n##### Основные типы сложности  \nВот основные виды сложности, которые встречаются чаще всего:  \n\n- **O(1)** — постоянная сложность. Независимо от размера данных, выполнение занимает одинаковое время. Например, получение элемента массива по индексу.  \n- **O(log N)** — логарифмическая сложность. Скорость растёт гораздо медленнее, чем сам объём данных. Хороший пример — бинарный поиск.  \n- **O(N)** — линейная сложность. Время выполнения прямо пропорционально объёму данных. Например, последовательный поиск в списке.  \n- **O(N²)** — квадратичная сложность. Каждый дополнительный элемент увеличивает время выполнения экспоненциально. Обычно возникает при наличии вложенных циклов.  \n- **O(N³)** — кубическая сложность. Ещё хуже, чем квадратичная. Возникает редко, но иногда бывает в трёхмерных задачах.  \n- **O(N!)** — факториальная сложность. Самый плохой сценарий, когда каждая операция требует полного перебора всех предыдущих шагов. Используется крайне редко.  \n\n##### Зачем нужна оценка сложности?  \nОценивая сложность своего алгоритма, вы можете:  \n\n- Предсказать, как поведёт себя программа при росте объёма данных.  \n- Определить узкие места и оптимизировать их.  \n- Подобрать подходящий алгоритм для вашей задачи.  \n\n#### Практическое применение  \n\nДопустим, у вас есть два списка чисел длиной `n`. Вам нужно найти общие элементы обоих списков. Рассмотрим два подхода:  \n\n```python\n# Подход №1: О(N²)\ndef find_common_elements(list1, list2):\n    common = []\n    for num in list1:       # O(N)\n        if num in list2:    # O(N)\n            common.append(num)\n    return common           # Общая сложность: O(N*N) = O(N²)\n```\n\nЭтот метод имеет квадратичную сложность, так как для каждой позиции первого списка проверяется вся длина второго списка.  \n\nТеперь посмотрим другой подход:  \n\n```python\n# Подход №2: О(N + M)\nfrom collections import Counter\n\ndef find_common_elements_optimized(list1, list2):\n    counter_list1 = Counter(list1)      # O(N)\n    result = [num for num in list2 if counter_list1[num]]  # O(M)\n    return result                       # Общая сложность: O(N+M)\n```\n\nВторой подход эффективнее, так как использует структуру данных (`Counter`), позволяющую проверять наличие элемента за постоянное время.  \n\n#### Практические советы  \n\n- Всегда оценивайте сложность ваших решений перед реализацией.  \n- Старайтесь избегать вложенных циклов там, где это возможно.  \n- Используйте подходящие структуры данных для ускорения поиска и вставки.  \n- Помните, что оптимизация важна, но чрезмерная преждевременная оптимизация тоже может привести к усложнению кода.  \n\n#### Заключение  \nЗнание сложности алгоритмов — важный навык для любого разработчика. Оно позволяет создавать быстрые и производительные приложения, способные справляться с большими нагрузками. Изучив эту концепцию, вы сможете уверенно подходить к решению любых задач, связанных с обработкой данных.  \n\n#### Дополнительные материалы  \n- Книга: Томас Х. Кормен, Чарльз И. Лейзерсон, Рональд Л. Ривест, Клифффорд Штайн — «Алгоритмы: построение и анализ».  \n- Онлайн-курс: Coursera — «Algorithms Specialization by Stanford University».  \n\nНадеюсь, этот материал оказался полезным и дал вам хорошее представление о важности понимания сложности алгоритмов. Удачного освоения материала!"
    },
    {
      "topic_id": "intermediate_01",
      "topic": "Бинарный поиск",
      "user_level": "intermediate",
      "success": true,
      "material_length": 4932,
      "word_count": 651,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.7,
      "documents_collected": 9,
      "completeness_score": 0.7,
      "relevance_score": 0.8,
      "quality_score": 0.7,
      "generation_time_seconds": 139.63900566101074,
      "gigachat2_calls": 8,
      "gigachat3_calls": 32,
      "estimated_cost_usd": 0.032,
      "tools_used": [
        "extract_concepts",
        "web_search",
        "adaptive_rag_search",
        "corrective_check"
      ],
      "generation_id": "gen_82c2b3714a14",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Учебный материал по теме «Бинарный поиск»\n\n## Уровень подготовки: Intermediate\n\n### 1. Введение\n\nПоиск данных является одной из важнейших операций в программировании. Часто возникает необходимость быстрого поиска элемента среди большого количества данных. Один из наиболее эффективных методов поиска — **бинарный поиск**. Этот алгоритм позволяет находить нужный элемент в отсортированном массиве значительно быстрее, чем обычный перебор.\n\nПочему важен бинарный поиск? Представьте себе ситуацию, когда вам нужно найти конкретный номер телефона в телефонной книге объемом в миллионы записей. Если искать простым перебором, это займет огромное количество времени. Но если применить бинарный поиск, результат можно получить почти мгновенно!\n\n### 2. Основная теория\n\n#### 2.1 Определение и принцип работы\n\n**Бинарный поиск** (binary search) — это эффективный алгоритм поиска элемента в отсортированной последовательности. Его основная идея заключается в следующем:\n\n- Сначала выбирается центральный элемент массива.\n- Затем сравнивается значение центрального элемента с искомым элементом.\n- Если центральное значение совпадает с искомым, поиск завершен.\n- Если искомое значение меньше центрального, дальнейший поиск продолжается в левой половине массива.\n- Если искомое значение больше центрального, поиск продолжается в правой половине массива.\n- Процесс повторяется, пока не найдется нужный элемент или пока область поиска не станет пустой.\n\nТаким образом, каждый шаг уменьшает размер области поиска вдвое, что делает алгоритм чрезвычайно эффективным.\n\n#### 2.2 Временная и пространственная сложность\n\n- **Временная сложность**: $O(\\log n)$, где $n$ — длина массива. Благодаря делению пополам на каждом шаге, количество шагов растет логарифмически относительно размера массива.\n  \n- **Пространственная сложность**: $O(1)$ (константная), поскольку используется лишь фиксированное количество переменных вне зависимости от размера массива.\n\n#### 2.3 Условия применения\n\nБинарный поиск возможен только в тех случаях, когда исходные данные отсортированы. Если массив не отсортирован, сначала потребуется провести сортировку, что увеличит общую временную сложность операции.\n\n### 3. Примеры\n\n#### 3.1 Поиск числа в отсортированном массиве\n\nДопустим, у нас есть отсортированный массив чисел:\n\n```python\narr = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\ntarget = 11\n```\n\nРеализуем бинарный поиск на Python:\n\n```python\ndef binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        mid = (left + right) // 2\n        \n        if arr[mid] == target:\n            return mid  # Возвращаем индекс найденного элемента\n        elif arr[mid] < target:\n            left = mid + 1  # Продолжаем поиск справа\n        else:\n            right = mid - 1  # Продолжаем поиск слева\n            \n    return None  # Элемента нет в массиве\n\nresult = binary_search(arr, target)\nprint(f'Элемент {target} находится на индексе {result}')\n```\n\nРезультатом выполнения будет:\n\n```\nЭлемент 11 находится на индексе 5\n```\n\n#### 3.2 Поиск минимального элемента в отсортированном повернутом массиве\n\nИногда массив изначально отсортирован, но был повёрнут (циклически сдвинут). Например:\n\n```python\narr = [15, 17, 19, 1, 3, 5, 7, 9, 11, 13]\n```\n\nЗдесь минимальный элемент — `1`. Реализация бинарного поиска для нахождения минимума:\n\n```python\ndef find_min_in_rotated_array(arr):\n    left, right = 0, len(arr) - 1\n    \n    while left < right:\n        mid = (left + right) // 2\n        \n        if arr[mid] > arr[right]:\n            left = mid + 1\n        else:\n            right = mid\n            \n    return arr[left]\n\nmin_value = find_min_in_rotated_array(arr)\nprint(f'Минимальный элемент: {min_value}')\n```\n\nРезультат:\n\n```\nМинимальный элемент: 1\n```\n\n### 4. Сравнение с аналогами\n\n| Метод поиска | Средняя сложность |\n|--------------|------------------|\n| Последовательный поиск | $O(n)$           |\n| Бинарный поиск         | $O(\\log n)$      |\n\nПреимущества бинарного поиска перед обычным перебором:\n- Значительно меньшая временная сложность ($O(\\log n)$ против $O(n)$).\n- Эффективность даже на огромных объемах данных.\n\nНедостатки:\n- Требует предварительно отсортированного массива.\n- Не подходит для динамических наборов данных, которые часто обновляются.\n\n### 5. Практические советы\n\n- Всегда убедитесь, что ваш массив отсортирован перед началом поиска.\n- Используйте встроенные функции библиотек (например, bisect в Python), если возможно.\n- Будьте осторожны с граничными условиями (массивы длиной 0 или 1).\n- Помните, что бинарный поиск неэффективен для небольших объемов данных (<10–20 элементов).\n\n### 6. Дополнительные ресурсы\n\nРекомендуемые материалы для углубленного изучения темы:\n- Лекции по алгоритмам и структурам данных (см. раздел \"Алгоритмы и структуры данных\").\n- Задачи на платформе LeetCode и Codeforces.\n\n---\n\nЭтот материал поможет разобраться в особенностях бинарного поиска и эффективно применять его на практике."
    },
    {
      "topic_id": "intermediate_02",
      "topic": "Быстрая сортировка QuickSort",
      "user_level": "intermediate",
      "success": true,
      "material_length": 4913,
      "word_count": 643,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.6,
      "documents_collected": 5,
      "completeness_score": 0.6,
      "relevance_score": 0.7,
      "quality_score": 0.5,
      "generation_time_seconds": 145.6409251689911,
      "gigachat2_calls": 9,
      "gigachat3_calls": 36,
      "estimated_cost_usd": 0.036,
      "tools_used": [
        "corrective_check",
        "adaptive_rag_search"
      ],
      "generation_id": "gen_f61adfd7c1f0",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Быстрая сортировка QuickSort  \n\n## Введение  \nБыстрая сортировка — один из наиболее эффективных методов упорядочивания данных среди существующих алгоритмов сортировки. Она широко применяется благодаря своей высокой производительности и относительной простоте реализации. Несмотря на потенциальную деградацию производительности в некоторых случаях, она остается одним из лучших вариантов для большинства реальных задач.  \n\nАлгоритм был предложен английским ученым Чарльзом Хоаром в 1960-х годах и быстро завоевал популярность благодаря своим характеристикам. Сегодня QuickSort является стандартом де-факто во многих языках программирования и библиотеках стандартных функций.  \n\n## Основная теория  \n\n### Принцип работы  \nБыстрая сортировка основана на методике разделения (\"разделяй и властвуй\"). Алгоритм работает следующим образом:  \n\n1. Выбирается опорный элемент (pivot). Обычно это первый, последний или средний элемент массива.  \n2. Массив делится на две части таким образом, чтобы элементы слева были меньше или равны pivot, а справа больше или равны. Этот этап называется разделением (partitioning).  \n3. Рекурсивно повторяется процедура для левой и правой частей массива, пока каждая часть не станет состоять из одного элемента.  \n\nПосле завершения рекурсии весь массив будет отсортированным.  \n\n### Пример пошагового выполнения  \nРассмотрим пример сортировки массива `[7, 2, 1, 6, 8, 5]`.  \n\n#### Шаг 1: Выбор опорного элемента  \nВыбираем `7` в качестве опорного элемента.  \n\n#### Шаг 2: Разделение массива  \nПереставляем элементы так, чтобы слева оказались числа меньше или равные `7`, а справа — большие:  \n```\n[2, 1, 6, 5 | 7 | 8]\n```  \n\n#### Шаг 3: Повторяем процедуру рекурсивно  \nТеперь применяем тот же подход отдельно к левым (`[2, 1, 6, 5]`) и правым (`[8]`) частям.  \n\nПродолжаем разделение, пока каждый сегмент не сократится до размера 1. Итоговый отсортированный массив выглядит так:  \n```\n[1, 2, 5, 6, 7, 8]\n```  \n\n### Временная и пространственная сложность  \n\n| Параметр             | Средняя сложность | Худшая сложность |\n|---------------------|--------------------|------------------|\n| Временная           | O(n log n)         | O(n²)            |\n| Пространственная    | O(log n)           | O(n)             |\n\nСреднее время работы составляет $O(n\\log{n})$, однако в худших сценариях (например, уже отсортированном массиве) производительность падает до квадратичной зависимости $O(n^2)$. Однако такие случаи редки, особенно при грамотном выборе опорного элемента.  \n\n## Практический пример  \nРеализуем алгоритм быстрой сортировки на Python:  \n\n```python\ndef quicksort(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    # выбираем опорный элемент\n    pivot = arr[len(arr) // 2]\n    \n    # делим массив на три группы\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    \n    # рекурсивно вызываем функцию для каждой части\n    return quicksort(left) + middle + quicksort(right)\n\n# тестируем\narr = [7, 2, 1, 6, 8, 5]\nprint(quicksort(arr))  # вывод: [1, 2, 5, 6, 7, 8]\n```\n\n### Объяснение кода  \n1. Если массив содержит один или ноль элементов, возвращаем его сразу.  \n2. Опорный элемент выбирается посередине массива.  \n3. Массив разбивается на три списка: элементы меньше опорного, равные и больше.  \n4. Применяем рекурсию к каждому сегменту и объединяем результаты.  \n\n## Сравнение с аналогами  \n\n### Другие популярные алгоритмы сортировки:  \n- **Merge Sort**: также обладает временем работы $O(n\\log{n})$ в среднем и худшем случаях, но требует дополнительной памяти.  \n- **Heap Sort**: обеспечивает гарантированное время $O(n\\log{n})$, но часто проигрывает QuickSort по скорости из-за большего количества обменов элементами.  \n\nПреимущества QuickSort:  \n- Высокая скорость в большинстве случаев.  \n- Минимальное использование дополнительной памяти.  \n\nНедостатки:  \n- Потенциальная деградация производительности до $O(n^2)$ в редких ситуациях.  \n\n## Практические советы  \n\n1. **Выбор хорошего опорного элемента**: Использование медианы трех значений (первого, среднего и последнего) помогает минимизировать вероятность попадания в плохие сценарии.  \n   \n2. **Оптимизация для небольших массивов**: Для маленьких сегментов лучше применять другие алгоритмы типа Insertion Sort, поскольку рекурсия становится неэффективной.  \n   \n3. **Параллелизм**: Реализация параллельной версии QuickSort позволяет значительно ускорить работу на многопроцессорных системах.  \n\n## Дополнительные ресурсы  \nРекомендуемые материалы для углубленного изучения темы:  \n- Книга \"Introduction to Algorithms\" by Thomas H. Cormen et al.  \n- Онлайн-курсы по алгоритмам и структурам данных на Coursera и Stepik.  \n\nТаким образом, QuickSort — мощный инструмент для решения задач сортировки, который эффективно справляется с большинством ситуаций, обеспечивая высокую производительность и минимальную нагрузку на оперативную память."
    },
    {
      "topic_id": "intermediate_03",
      "topic": "Хеш-таблицы как структура данных",
      "user_level": "intermediate",
      "success": true,
      "material_length": 6564,
      "word_count": 783,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.65,
      "documents_collected": 5,
      "completeness_score": 0.65,
      "relevance_score": 0.9,
      "quality_score": 0.7,
      "generation_time_seconds": 154.7112638950348,
      "gigachat2_calls": 9,
      "gigachat3_calls": 36,
      "estimated_cost_usd": 0.036,
      "tools_used": [
        "web_search",
        "adaptive_rag_search"
      ],
      "generation_id": "gen_f801b2bfd171",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Учебный материал  \n## Хеш-таблицы как структура данных  \n\n### Уровень подготовки: Intermediate  \n\n#### Автор: GigaChat  \n\n---\n\n## Введение  \n\nХеш-таблица — это структура данных, позволяющая эффективно хранить и быстро извлекать данные по уникальному идентификатору, называемому ключом. Она широко используется там, где важна высокая скорость операций поиска, добавления и удаления элементов. Например, базы данных, кэш-память, компиляторы и интерпретаторы используют хеш-таблицы для быстрого доступа к данным. \n\nЭффективность хеш-таблиц обусловлена использованием специальной функции — хеш-функции, которая преобразует ключи в индексы массива. Однако, поскольку разные ключи могут давать одинаковые значения индекса (коллизии), существуют различные способы обработки таких ситуаций.  \n\n---\n\n## Основная теория  \n\n### Концепция хеш-таблиц  \n\nХеш-таблица представляет собой массив фиксированного размера (`HASHSIZE`), каждый элемент которого содержит либо сам объект (ключ-значение), либо ссылку на структуру данных, хранящую такие объекты. Для преобразования ключей в индексы массива используется специальная функция — хеш-функция.  \n\nНапример, пусть размер нашей хеш-таблицы равен `10`, а хеш-функция возвращает остаток от деления ключа на этот размер (`key % HASHSIZE`):  \n\n```python\ndef hash_function(key):\n    return key % 10\n```\n\nТогда ключ `\"Москва\"` (числовое представление строки) будет размещён в ячейке с номером `hash_function(\"Москва\")`.  \n\n### Обработка коллизий  \n\nКоллизией называется ситуация, когда два разных ключа отображаются одной и той же хеш-функцией в один и тот же индекс массива. Существует два основных подхода к обработке коллизий:  \n\n1. **Открытая адресация**: элементы хранятся непосредственно в самом массиве, а при возникновении коллизии используются специальные техники размещения новых элементов (например, линейное зондирование, квадратичное зондирование, двойное хеширование).  \n   \n2. **Закрытая адресация (метод цепочек)**: каждая ячейка массива содержит указатель на связанный список (или другое дерево поиска), куда добавляются конфликтующие элементы. Этот метод позволяет легко добавлять новые элементы, однако увеличивает накладные расходы памяти.  \n\n### Пример закрытой адресации  \n\nРассмотрим пример хеш-таблицы размером `5`, используя закрытую адресацию методом цепочек:  \n\n| Индекс | Элемент |\n|--------|---------|\n| 0      | NULL    |\n| 1      | [\"Ключ1\"] -> [\"Ключ2\"] |\n| 2      | NULL    |\n| 3      | [\"Ключ3\"] |\n| 4      | NULL    |\n\nЗдесь при попытке добавить новый элемент с ключом, дающим тот же индекс, происходит создание нового узла в связанном списке.  \n\n### Временная и пространственная сложность  \n\nИдеальная хеш-таблица имеет временную сложность операций поиска, вставки и удаления равную O(1) в среднем случае. Однако в худшем сценарии (при большом количестве коллизий) эти операции могут деградировать до O(n).  \n\nПространственная сложность зависит от выбранного метода разрешения коллизий:  \n\n- Открытая адресация требует меньше дополнительной памяти, но сложнее обрабатывается при высокой плотности заполнения.  \n- Закрытая адресация потребляет больше памяти за счёт связанных списков, но проще реализуется и масштабируется лучше.  \n\n---\n\n## Практика  \n\n### Пример №1: Реализация хеш-таблицы с закрытым адресованием  \n\nРеализуем простую хеш-таблицу с разрешением коллизий методом цепочек на Python:  \n\n```python\nclass HashTable:\n    def __init__(self, size=10):\n        self.size = size\n        # Массив ссылок на связанные списки\n        self.table = [None for _ in range(size)]\n    \n    def hash_function(self, key):\n        return sum(bytearray(key.encode())) % self.size\n    \n    def insert(self, key, value):\n        index = self.hash_function(key)\n        \n        if not self.table[index]:\n            # Создаём новую цепь\n            self.table[index] = [(key, value)]\n        else:\n            # Добавляем в существующую цепь\n            self.table[index].append((key, value))\n    \n    def search(self, key):\n        index = self.hash_function(key)\n        chain = self.table[index]\n        \n        if chain is None:\n            return None\n        \n        for k, v in chain:\n            if k == key:\n                return v\n        return None\n    \n    def delete(self, key):\n        index = self.hash_function(key)\n        chain = self.table[index]\n        \n        if chain is None:\n            return False\n        \n        for i, pair in enumerate(chain):\n            if pair[0] == key:\n                del chain[i]\n                return True\n        return False\n```\n\n### Объяснение  \n\n1. Мы инициализировали таблицу заданного размера, заполнив её значениями `None`.\n2. Функция `insert()` добавляет парные записи `{ключ, значение}` в соответствующую позицию массива.\n3. Метод `search()` ищет значение по указанному ключу, проходя по связанной цепи.\n4. Операция `delete()` удаляет запись, находя её в соответствующей цепи.  \n\n---\n\n## Сравнение с аналогами  \n\n### Другие структуры данных  \n\n- **Массивы**: обеспечивают быстрый доступ по индексу, но требуют заранее известного количества элементов и неэффективны при частых операциях вставки и удаления.  \n- **Связанные списки**: поддерживают произвольные вставки и удаление, но имеют медленный доступ к элементам (O(n)).  \n- **Деревья поиска**: предоставляют эффективный баланс скорости поиска и модификации, но занимают больше памяти и сложны в реализации.  \n\nПреимущества хеш-таблиц заключаются в быстрой работе большинства операций (O(1)) и относительной простоте реализации. Основные недостатки — необходимость хорошей хеш-функции и проблемы с обработкой коллизий.  \n\n---\n\n## Практические советы  \n\n1. **Выбирайте хорошую хеш-функцию**. Чем равномернее распределение значений, тем ниже вероятность коллизий.  \n2. **Используйте подходящий коэффициент загрузки**. Поддерживайте нагрузку на уровне ~0.7–0.8, чтобы минимизировать риск замедления операций.  \n3. **Избегайте чрезмерного увеличения размеров таблиц**, иначе вы потеряете преимущества эффективности.  \n4. **Рассмотрите использование готовых библиотек**. Многие языки программирования уже содержат эффективные реализации хеш-таблиц (например, словарь `dict` в Python).  \n\n---\n\n## Дополнительные ресурсы  \n\n- Книга: Thomas H. Cormen et al., \"Introduction to Algorithms\".  \n- Курс: \"Algorithms, Part I\" by Robert Sedgewick on Coursera.  \n- Документация: Официальная документация Python по встроенным типам данных.  \n\n---\n\nЭтот материал подготовлен для уровня intermediate и охватывает основные аспекты хеш-таблиц, позволяя глубже разобраться в этой важной структуре данных."
    },
    {
      "topic_id": "intermediate_04",
      "topic": "Двоичное дерево поиска BST",
      "user_level": "intermediate",
      "success": true,
      "material_length": 6270,
      "word_count": 797,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.6,
      "documents_collected": 5,
      "completeness_score": 0.6,
      "relevance_score": 0.8,
      "quality_score": 0.7,
      "generation_time_seconds": 195.13350558280945,
      "gigachat2_calls": 8,
      "gigachat3_calls": 32,
      "estimated_cost_usd": 0.032,
      "tools_used": [
        "extract_concepts",
        "web_search",
        "adaptive_rag_search"
      ],
      "generation_id": "gen_0fac02e0e3b5",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Учебный материал по теме «Двоичное дерево поиска»\n\n## Введение\n\nДвоичное дерево поиска (BST, Binary Search Tree) — это специальная структура данных, предназначенная для эффективного хранения и обработки упорядоченных наборов данных. Она позволяет быстро находить, вставлять и удалять элементы, обеспечивая среднее время выполнения операций порядка O(log n). \n\nЭта структура широко используется во многих приложениях, включая базы данных, файловые системы, компиляторы и другие программы, где важна быстрая обработка больших объемов данных. Например, в базах данных деревья используются для индексации таблиц, ускоряя выборку нужных строк.\n\n## Основная теория\n\n### Определение\n\nДвоичное дерево поиска — это такое двоичное дерево, в котором выполняется следующее свойство:\n\n- Все элементы левого поддерева узла строго меньше значения узла.\n- Все элементы правого поддерева узла строго больше значения узла.\n\nЭто свойство обеспечивает возможность быстрого поиска нужного элемента путем последовательного сужения области поиска.\n\n### Представление в памяти\n\nКаждый узел дерева содержит:\n\n```python\nclass Node:\n    def __init__(self, key):\n        self.key = key       # ключ узла\n        self.left = None     # указатель на левого сына\n        self.right = None    # указатель на правого сына\n```\n\nИногда полезно хранить ссылку на родителя (`parent`), хотя она необязательна.\n\n### Основные операции\n\n#### 1. Поиск элемента\n\nАлгоритм поиска работает следующим образом:\n\n- Начинаем с корня.\n- Если текущее значение совпадает с искомым, останавливаемся.\n- Иначе, если искомое значение меньше текущего, продолжаем поиск в левом поддереве.\n- Если больше — в правом.\n\nВремя работы: $O(\\log n)$ в среднем, $O(n)$ в худшем случае (вырожденное дерево).\n\nКод поиска:\n\n```python\ndef search(root, target):\n    if root is None or root.key == target:\n        return root\n    elif target < root.key:\n        return search(root.left, target)\n    else:\n        return search(root.right, target)\n```\n\n#### 2. Минимум и максимум\n\n- Чтобы найти минимум, спускаемся по левым ветвям до упора.\n- Чтобы найти максимум, спускаемся по правым ветвям до упора.\n\nВремя работы: $O(\\log n)$.\n\n```python\ndef find_minimum(root):\n    while root.left is not None:\n        root = root.left\n    return root\n\ndef find_maximum(root):\n    while root.right is not None:\n        root = root.right\n    return root\n```\n\n#### 3. Следующий и предыдущий элемент\n\nСледующим элементом после узла `x` считается минимальное значение среди тех, кто больше `x`. И наоборот, предыдущим — максимальное значение среди тех, кто меньше `x`.\n\nВремя работы: $O(\\log n)$.\n\n```python\ndef successor(node):\n    if node.right is not None:\n        return find_minimum(node.right)\n    \n    parent = node.parent\n    while parent is not None and node == parent.right:\n        node = parent\n        parent = parent.parent\n    return parent\n```\n\n#### 4. Вставка нового элемента\n\nПроцесс похож на поиск: спускаемся вниз по дереву, выбирая направление в зависимости от значения ключа. После достижения листа добавляем новый узел.\n\nВремя работы: $O(\\log n)$.\n\n```python\ndef insert(root, new_key):\n    if root is None:\n        return Node(new_key)\n    \n    if new_key < root.key:\n        root.left = insert(root.left, new_key)\n        root.left.parent = root\n    else:\n        root.right = insert(root.right, new_key)\n        root.right.parent = root\n        \n    return root\n```\n\n#### 5. Удаление элемента\n\nУдаление сложнее остальных операций, так как нужно поддерживать балансировку дерева. Возможны несколько случаев:\n\n- Лист удаляется простым сбросом ссылок.\n- У узла один ребенок — заменяем ссылкой на ребенка.\n- У узла два ребенка — заменяем его ближайшим преемником (например, минимальными значениями из правого поддерева).\n\nВремя работы: $O(\\log n)$.\n\n```python\ndef delete_node(root, key):\n    if root is None:\n        return root\n    \n    if key < root.key:\n        root.left = delete_node(root.left, key)\n    elif key > root.key:\n        root.right = delete_node(root.right, key)\n    else:\n        if root.left is None:\n            temp = root.right\n            root = None\n            return temp\n        elif root.right is None:\n            temp = root.left\n            root = None\n            return temp\n            \n        temp = find_minimum(root.right)\n        root.key = temp.key\n        root.right = delete_node(root.right, temp.key)\n    \n    return root\n```\n\n### Время и пространство\n\n| Операция      | Среднее время | Худшее время |\n|---------------|---------------|--------------|\n| Поиск         | O(log n)      | O(n)         |\n| Вставка       | O(log n)      | O(n)         |\n| Удаление      | O(log n)      | O(n)         |\n| Найти min/max | O(log n)      | O(n)         |\n\nПространство: O(n).\n\n## Практика\n\nРассмотрим пример простого дерева поиска:\n\n```\n      5\n     / \\\n    3   8\n   / \\   \n  2   4  \n```\n\nДопустим, нам нужно найти элемент 4. Мы начинаем с корня (5), затем двигаемся влево (3), далее вправо (4) и находим нужный элемент.\n\nТеперь попробуем удалить элемент 3. Так как у него есть оба ребёнка, мы найдем ближайшего наследника (4), перенесем его на место удалённого узла и обновим связи.\n\n## Сравнение с аналогами\n\n- **Связанный список**: медленнее (O(n)) для большинства операций.\n- **Отсортированные массивы**: быстрее для поиска (O(log n)), но медленные изменения структуры (O(n)).\n- **Самобалансирующиеся деревья**: обеспечивают гарантированное время O(log n), но требуют дополнительной логики для поддержания баланса.\n\n## Практические советы\n\n1. Всегда проверяйте высоту дерева: вырожденные деревья сильно замедляют работу.\n2. Используйте самобалансирующиеся варианты (AVL, красно-чёрные деревья), когда нужна гарантия производительности.\n3. Не забывайте про обработку граничных случаев (листья, пустые деревья).\n4. Храните дополнительную информацию (родителей, размеры поддеревьев) для ускорения некоторых операций.\n\n## Дополнительные материалы\n\n- Википедия: https://ru.wikipedia.org/wiki/%D0%94%D0%B2%D0%BE%D0%B8%D1%87%D0%BD%D0%BE%D0%B5_%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE_%D0%BF%D0%BE%D0%B8%D1%81%D0%BA%D0%B0\n- Книга \"Introduction to Algorithms\" by Thomas H. Cormen et al.\n\nЭтот материал поможет вам освоить основы работы с двоичными деревьями поиска и эффективно применять их в реальных проектах."
    },
    {
      "topic_id": "intermediate_05",
      "topic": "Рекурсия и обход дерева",
      "user_level": "intermediate",
      "success": true,
      "material_length": 7584,
      "word_count": 910,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.4,
      "documents_collected": 7,
      "completeness_score": 0.4,
      "relevance_score": 0.5,
      "quality_score": 0.6,
      "generation_time_seconds": 151.28690266609192,
      "gigachat2_calls": 9,
      "gigachat3_calls": 36,
      "estimated_cost_usd": 0.036,
      "tools_used": [
        "web_search",
        "corrective_check",
        "adaptive_rag_search"
      ],
      "generation_id": "gen_37c7b21ddcdb",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Рекурсия и обход дерева  \n\n## Введение  \nДеревья — одна из важнейших структур данных в информатике, широко применяемая во многих областях программирования. Обход дерева — это процесс последовательного посещения всех его элементов. Существует несколько методов обхода, каждый из которых имеет свою специфику и область применения. Одним из наиболее эффективных подходов является использование рекурсии, позволяющей элегантно и эффективно решать задачи обработки древовидных структур.  \n\nРекурсия позволяет легко реализовать алгоритмы обхода дерева, такие как прямой, симметричный и обратный обходы. Эти методы позволяют последовательно обрабатывать узлы дерева, выполняя необходимые операции (например, вывод значений, вычисления и другие манипуляции).  \n\nКроме того, существуют специальные структуры, такие как декартовы деревья, позволяющие оптимизировать работу с деревьями поиска благодаря использованию дополнительного ключа сортировки. Это делает возможным эффективное выполнение операций поиска, вставки и удаления элементов.  \n\n## Основная теория  \n\n### Основные понятия  \n\n*Дерево* — это иерархическая структура данных, состоящая из узлов (*вершины*) и связей между ними (*ребра*). Каждый узел содержит данные и ссылки на дочерние узлы. У дерева есть один специальный узел — *корень*, который не имеет родителя. Остальные узлы называются листьями, если у них нет детей, либо внутренними вершинами, если имеют хотя бы одного ребёнка.  \n\nОбход дерева — это процедура, при которой каждая вершина посещается ровно один раз. Существуют три основные стратегии обхода:  \n\n- Прямой (preorder): обработка узла → левое поддерево → правое поддерево  \n- Симметричный (inorder): левое поддерево → обработка узла → правое поддерево  \n- Обратный (postorder): левое поддерево → правое поддерево → обработка узла  \n\nЭти способы отличаются порядком выполнения действий относительно текущего узла и его поддеревьев.  \n\n### Реализация через рекурсию  \n\nДля реализации обхода удобно использовать рекурсивную функцию, которая вызывает сама себя для обработки поддеревьев. Рассмотрим реализацию прямого обхода на примере бинарного дерева поиска:  \n\n```python\nclass TreeNode:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\ndef preorder_traversal(node):\n    if node is not None:\n        # Сначала обрабатываем текущий узел\n        print(node.value)\n        \n        # Затем переходим к левому поддереву\n        preorder_traversal(node.left)\n        \n        # И наконец к правому поддереву\n        preorder_traversal(node.right)\n```\n\nЭтот код демонстрирует принцип рекурсивного обхода: функция `preorder_traversal` вызывается сначала для корня, затем для его левого и правого поддеревьев.  \n\n### Время и пространство  \n\nВремя выполнения каждой из стратегий обхода составляет $O(n)$, где $n$ — количество узлов в дереве. Пространство также линейно зависит от глубины дерева ($O(h)$), поскольку рекурсивные вызовы занимают место в стеке.  \n\n## Практический пример: реализация обхода дерева  \n\nРассмотрим простую структуру бинарного дерева поиска и реализуем все три способа обхода:  \n\n```python\nclass BinaryTreeNode:\n    def __init__(self, key):\n        self.key = key\n        self.left = None\n        self.right = None\n\ndef insert(root, key):\n    \"\"\"Функция вставки элемента в бинарное дерево\"\"\"\n    if root is None:\n        return BinaryTreeNode(key)\n    \n    if key < root.key:\n        root.left = insert(root.left, key)\n    else:\n        root.right = insert(root.right, key)\n    \n    return root\n\n# Создаём дерево\nroot = None\nkeys = [10, 5, 15, 3, 7, 12, 18]\nfor k in keys:\n    root = insert(root, k)\n\n# Функция вывода результата обхода\ndef print_tree_traversal(traversal_type):\n    traversal_type(root)\n    print()\n\n# Реализуем три типа обхода\ndef preorder(node):\n    if node:\n        print(node.key, end=\" \")\n        preorder(node.left)\n        preorder(node.right)\n\ndef inorder(node):\n    if node:\n        inorder(node.left)\n        print(node.key, end=\" \")\n        inorder(node.right)\n\ndef postorder(node):\n    if node:\n        postorder(node.left)\n        postorder(node.right)\n        print(node.key, end=\" \")\n\nprint(\"Прямой обход:\")\nprint_tree_traversal(preorder)\n\nprint(\"Симметричный обход:\")\nprint_tree_traversal(inorder)\n\nprint(\"Обратный обход:\")\nprint_tree_traversal(postorder)\n```\n\nРезультатом выполнения программы будет следующее:  \n\n```\nПрямой обход:\n10 5 3 7 15 12 18 \nСимметричный обход:\n3 5 7 10 12 15 18 \nОбратный обход:\n3 7 5 12 18 15 10 \n```\n\nТаким образом, мы видим различия в порядке вывода ключей в зависимости от выбранного метода обхода.  \n\n## Специальные структуры: Декартово дерево  \n\nПомимо стандартных деревьев поиска, существуют специализированные структуры, такие как декартово дерево (`treap`), которое сочетает в себе свойства бинарного дерева поиска и бинарной кучи. Оно обладает двумя ключевыми особенностями:  \n\n- Элементы сортируются по первому ключу (как в BST), обеспечивая быстрый доступ и поиск.  \n- Второй ключ используется для поддержания свойств кучи, гарантирующих сбалансированность дерева.  \n\nЭто обеспечивает среднее время выполнения операций поиска, вставки и удаления порядка $O(\\log n)$.  \n\nПример реализации вставки в декартовом дереве выглядит следующим образом:  \n\n```python\nimport random\n\nclass TreapNode:\n    def __init__(self, key):\n        self.key = key\n        self.priority = random.random()  # Случайный приоритет\n        self.left = None\n        self.right = None\n\ndef split(node, x):\n    \"\"\"Разделение дерева на два поддерева по значению x\"\"\"\n    if node is None:\n        return None, None\n    \n    if node.key <= x:\n        left, right = split(node.right, x)\n        node.right = left\n        return node, right\n    else:\n        left, right = split(node.left, x)\n        node.left = right\n        return left, node\n\ndef merge(left, right):\n    \"\"\"Объединение двух деревьев в одно\"\"\"\n    if left is None or right is None:\n        return left or right\n    \n    if left.priority > right.priority:\n        left.right = merge(left.right, right)\n        return left\n    else:\n        right.left = merge(left, right.left)\n        return right\n\ndef treap_insert(root, key):\n    new_node = TreapNode(key)\n    left, right = split(root, key)\n    return merge(merge(left, new_node), right)\n\n# Пример использования\nroot = None\nfor i in range(10):\n    root = treap_insert(root, i)\n```\n\nЗдесь мы используем две вспомогательные функции: `split`, разделяющую дерево на части, и `merge`, объединяющую эти части после вставки нового узла.  \n\n## Практические советы  \n\n1. **Избегать бесконечной рекурсии**: убедитесь, что условие выхода из рекурсии корректно задано.  \n   \n2. **Используйте стек вручную**, если глубина дерева велика, чтобы избежать переполнения стека.  \n   \n3. **Оптимизация памяти**: старайтесь минимизировать создание новых объектов при обработке больших деревьев.  \n   \n4. **Тестируйте разные случаи**: проверяйте поведение вашего решения на пустых деревьях, единичных узлах и несбалансированных структурах.  \n   \n5. **Анализ производительности**: оценивайте временную и пространственную сложность ваших решений перед внедрением в производственный код.  \n\n## Заключение  \n\nМы рассмотрели основы рекурсивного обхода деревьев, изучили различные подходы и реализовали их на практике. Мы также познакомились с продвинутым подходом — использованием декартовых деревьев, обеспечивающих высокую производительность операций поиска и модификации.  \n\nТеперь вы готовы применять полученные знания для эффективного решения задач, связанных с обработкой древовидных структур данных."
    },
    {
      "topic_id": "advanced_01",
      "topic": "Динамическое программирование",
      "user_level": "advanced",
      "success": true,
      "material_length": 3568,
      "word_count": 402,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.7,
      "documents_collected": 10,
      "completeness_score": 0.7,
      "relevance_score": 1.0,
      "quality_score": 0.7,
      "generation_time_seconds": 153.98825025558472,
      "gigachat2_calls": 9,
      "gigachat3_calls": 36,
      "estimated_cost_usd": 0.036,
      "tools_used": [
        "web_search",
        "adaptive_rag_search",
        "corrective_check"
      ],
      "generation_id": "gen_8082d5205a8d",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Динамическое программирование  \n\n## Введение  \n\nДинамическое программирование — мощный метод оптимизации решений задач путём декомпозиции исходной проблемы на серию взаимосвязанных подзадач. Этот подход позволяет эффективно избегать повторного пересчета одних и тех же промежуточных результатов, существенно сокращая вычислительную нагрузку. Метод широко используется в задачах оптимизации, обработки строк, анализа последовательностей и многих других областях.\n\n## Основная теория  \n\n### Суть метода  \nМетод динамического программирования основан на следующих ключевых принципах:  \n\n1. **Выделение подзадач**: Исходная задача разделяется на серию связанных подзадач, каждая из которых решается независимо.  \n2. **Оптимальная подструктура**: Решение каждой подзадачи должно зависеть лишь от решений предыдущих подзадач.  \n3. **Запоминание результатов**: Результаты уже решённых подзадач сохраняются, предотвращая повторные расчёты.  \n4. **Рекуррентные соотношения**: Между результатами подзадач существуют чётко определённые зависимости, выражаемые формулами или функциями.  \n\n### Пример задачи  \nРассмотрим классическую задачу нахождения наибольшей общей подпоследовательности двух строк (`LCS`). Пусть даны строки `X = x₁x₂...xᵢ` и `Y = y₁y₂...yᵤ`. Необходимо определить максимальную длину последовательности символов, встречающуюся одновременно в обеих строках.  \n\n#### Рекуррентное соотношение  \nПусть `dp[i][j]` обозначает длину наибольшей общей подпоследовательности первых i символов X и j символов Y. Тогда:  \n\n$$\ndp[i][j] =\n\\begin{cases}\n0 & \\text{if } i=0 \\text{ or } j=0 \\\\\ndp[i-1][j-1]+1 & \\text{if } x_i=y_j \\\\\n\\max(dp[i-1][j], dp[i][j-1]) & \\text{otherwise}\n\\end{cases}\n$$  \n\n### Реализация на Python  \n```python\ndef lcs(X, Y):\n    m, n = len(X), len(Y)\n    # Создаем таблицу размером (m+1)x(n+1)\n    dp = [[0 for _ in range(n+1)] for __ in range(m+1)]\n    \n    # Заполняем таблицу снизу вверх\n    for i in range(1, m+1):\n        for j in range(1, n+1):\n            if X[i-1] == Y[j-1]:\n                dp[i][j] = dp[i-1][j-1] + 1\n            else:\n                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n                \n    return dp[m][n]\n```\n\n### Анализ временной и пространственной сложности  \n- Время выполнения: O(m*n), где m и n длины строк соответственно.  \n- Пространственная сложность: O(m*n).\n\n## Практическое применение  \n\n### Примеры реальных задач  \n1. **Поиск оптимального маршрута**: Нахождение кратчайшего пути в транспортной сети.  \n2. **Анализ ДНК-последовательностей**: Поиск сходства между генетическими цепочками.  \n3. **Распознавание образов**: Определение схожих изображений или объектов.  \n\n### Советы по применению  \n- Всегда проверяйте наличие оптимальной подструктуры перед применением динамического программирования.  \n- Используйте двумерные таблицы для хранения промежуточных результатов.  \n- Убедитесь, что размер массива достаточен для хранения всех необходимых значений.  \n\n## Дополнительные ресурсы  \n- [Кормен, Лейзерсон, Ривест, Штайн. Алгоритмы: построение и анализ](https://www.labirint.ru/books/47111/)  \n- [Кнут Дональд Эрвин. Искусство программирования. Том 1: Основные алгоритмы](https://www.ozon.ru/product/iskusstvo-programmirovaniya-tom-1-osnovnye-algoritmy-knut-donald-erwin-13616663/)\n\n## Заключение  \nДинамическое программирование — универсальный инструмент для эффективного решения широкого спектра задач оптимизации. Его правильное применение требует понимания принципов декомпозиции и сохранения промежуточных результатов, что делает его мощным инструментом в арсенале профессионального разработчика."
    },
    {
      "topic_id": "advanced_02",
      "topic": "Алгоритм Дейкстры",
      "user_level": "advanced",
      "success": true,
      "material_length": 5011,
      "word_count": 584,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.4,
      "documents_collected": 3,
      "completeness_score": 0.4,
      "relevance_score": 0.7,
      "quality_score": 0.7,
      "generation_time_seconds": 133.84849977493286,
      "gigachat2_calls": 8,
      "gigachat3_calls": 32,
      "estimated_cost_usd": 0.032,
      "tools_used": [
        "extract_concepts",
        "adaptive_rag_search",
        "web_search",
        "corrective_check"
      ],
      "generation_id": "gen_d84149322bd7",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Алгоритм Дейкстры — продвинутый уровень\n\n## Введение\n\nАлгоритм Дейкстры — один из наиболее известных методов поиска кратчайшего пути в графах с неотрицательными весами рёбер. Разработан нидерландским учёным Эдсгером Дейкстрой в 1959 году. Этот алгоритм широко используется в различных областях информатики и компьютерных технологий, включая маршрутизацию сетей, навигационные системы, планирование ресурсов и многие другие задачи оптимизации.\n\nЭффективность и универсальность делают алгоритм Дейкстры незаменимым инструментом для специалистов, работающих над проектами, связанными с обработкой графовых моделей и поиском эффективных путей.\n\n## Основная теория\n\n### Концепция и принцип работы\n\nАлгоритм Дейкстры решает задачу поиска кратчайшего пути от одной вершины графа до остальных. Его основная идея основана на постепенном расширении множества посещённых вершин, начиная с начальной вершины, и поддержании расстояния до каждой вершины в виде минимальной стоимости достижения этой вершины.\n\nКлючевая особенность алгоритма — использование приоритетной очереди (обычно реализованной через кучу), где вершинам присваиваются приоритеты, соответствующие текущим оценкам расстояний.\n\n#### Основные шаги алгоритма:\n\n1. Инициализация:\n   - Все вершины помечаются как непосещённые.\n   - Расстояние до стартовой вершины устанавливается равным нулю, остальные — бесконечности.\n   \n2. Основной цикл:\n   - Выбирается ближайшая непосещённая вершина.\n   - Для выбранной вершины пересчитываются расстояния до соседних вершин.\n   - Вершина отмечается как посещённая.\n   \n3. Повторение цикла до посещения всех вершин или достижения целевой вершины.\n\n### Формальное описание\n\nПусть дан ориентированный взвешенный граф $G=(V,E)$, где $V$ — множество вершин, $E$ — множество рёбер, каждое ребро $(u,v)$ имеет вес $w(u,v)$. Необходимо найти кратчайшие пути от исходной вершины $s$ ко всем остальным вершинам.\n\nАлгоритм работает следующим образом:\n\n```python\ndef dijkstra(graph, start):\n    # Инициализация расстояний и предыдущих вершин\n    distances = {vertex: float('inf') for vertex in graph}\n    previous_vertices = {vertex: None for vertex in graph}\n    \n    distances[start] = 0\n    priority_queue = [(0, start)]\n    \n    while priority_queue:\n        current_distance, current_vertex = heapq.heappop(priority_queue)\n        \n        if current_distance > distances[current_vertex]:\n            continue\n            \n        for neighbor, weight in graph[current_vertex].items():\n            distance = current_distance + weight\n            \n            if distance < distances[neighbor]:\n                distances[neighbor] = distance\n                previous_vertices[neighbor] = current_vertex\n                heapq.heappush(priority_queue, (distance, neighbor))\n                \n    return distances, previous_vertices\n```\n\n### Временная и пространственная сложность\n\n- **Время работы**: $O((|V|+|E|)\\log|V|)$ при использовании бинарной кучи.\n- **Память**: $O(|V|)$.\n\n## Примеры\n\nРассмотрим пример простого графа с пятью вершинами и положительными весами рёбер:\n\n```python\ngraph = {\n    'A': {'B': 1, 'C': 4},\n    'B': {'A': 1, 'C': 2, 'D': 5},\n    'C': {'A': 4, 'B': 2, 'D': 1},\n    'D': {'B': 5, 'C': 1}\n}\nstart_vertex = 'A'\ndistances, path = dijkstra(graph, start_vertex)\nprint(\"Кратчайшие расстояния:\", distances)\nprint(\"Предыдущие вершины:\", path)\n```\n\nРезультатом будет словарь кратчайших расстояний и путь восстановления маршрута.\n\n## Сравнение с аналогами\n\n### Bellman-Ford vs Dijkstra\n\n- **Bellman-Ford** позволяет находить кратчайшие пути даже в графах с отрицательными весами рёбер, однако его временная сложность значительно хуже ($O(|V|\\cdot |E|)$).\n  \n- **Dijkstra** быстрее, но требует неотрицательных весов рёбер.\n\n### A* vs Dijkstra\n\n- **A*** добавляет эвристику, ускоряя процесс поиска путём оценки перспективности направлений движения. Однако корректность результата зависит от качества эвристики.\n\n- **Dijkstra** гарантированно находит оптимальный путь, но не учитывает дополнительную информацию о структуре графа.\n\n## Практические советы\n\n1. Всегда проверяйте наличие циклов с отрицательным весом перед применением алгоритма.\n2. Используйте эффективные структуры данных (например, очередь с приоритетом) для ускорения вычислений.\n3. Обновляйте расстояние до вершины только тогда, когда оно действительно уменьшается.\n4. Не забывайте учитывать граничные случаи (графы с одной вершиной, пустые графы).\n5. Реализация алгоритма на практике часто включает дополнительные проверки и обработки ошибок.\n\n## Дополнительные ресурсы\n\n- [Специализация \"Алгоритмы и структуры данных\"](https://example.com/algorithms-specialization)\n- [Курс Алексея Панфилова](https://stepik.org/course/algorithm-course)\n- Книга \"Introduction to Algorithms\" by Thomas H. Cormen et al.\n\nТаким образом, понимание и применение алгоритма Дейкстры открывает широкие возможности для эффективного решения задач поиска кратчайших путей в графах, делая его важным элементом арсенала любого специалиста по алгоритмам и структурам данных."
    },
    {
      "topic_id": "advanced_03",
      "topic": "AVL-деревья",
      "user_level": "advanced",
      "success": true,
      "material_length": 6945,
      "word_count": 854,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.6,
      "documents_collected": 9,
      "completeness_score": 0.6,
      "relevance_score": 0.7,
      "quality_score": 0.7,
      "generation_time_seconds": 189.26531195640564,
      "gigachat2_calls": 9,
      "gigachat3_calls": 36,
      "estimated_cost_usd": 0.036,
      "tools_used": [
        "web_search",
        "adaptive_rag_search",
        "corrective_check"
      ],
      "generation_id": "gen_0c2b1b60dfc0",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Учебный материал по AVL-деревьям  \n\n## Введение  \n\nAVL-дерево — это разновидность бинарного дерева поиска, названная в честь советских ученых Г. М. Адельсона-Вельского и Е. М. Ланди́са, предложивших эту структуру в 1962 году. Основная цель AVL-дерева — обеспечить высокую эффективность операций поиска, добавления и удаления элементов путем поддержания баланса дерева.  \n\nПочему это важно?  \n\n- Высокая скорость доступа к данным ($O(\\log n)$) даже в худшем случае.  \n- Устойчивая производительность независимо от последовательности вставляемых элементов.  \n- Широкая область применения: базы данных, файловые системы, компиляторы, поисковые движки и другие приложения, где важна высокая скорость обработки запросов.  \n\nГде применяется?  \n\n- Хранение больших объемов упорядоченных данных.  \n- Реализация эффективных алгоритмов поиска и сортировки.  \n- Поддержание индексации в системах управления базами данных.  \n- Использование в качестве основы для более сложных структур данных.  \n\n## Основная теория  \n\n### Определение и принцип работы  \n\nAVL-дерево — это самоорганизующаяся структура данных, представляющая собой бинарное дерево поиска, которое поддерживает балансировку своей структуры. Основное свойство AVL-дерева заключается в том, что абсолютная разница высот левого и правого поддеревьев любого узла не превышает единицы.  \n\nФормальное определение:  \n\n> Высота AVL-дерева определяется как максимальная длина пути от корня до листа. Условие балансировки гласит, что для каждого узла $|h_{левое}-h_{правое}| \\leq 1$, где $h$ — высота соответствующего поддерева.  \n\n### Операции над AVL-деревом  \n\n#### 1. Поиск элемента  \nПоиск выполняется стандартным способом, как в обычном бинарном дереве поиска. Время поиска — $O(\\log n)$.  \n\n#### 2. Вставка нового элемента  \nПосле стандартной вставки проверяется условие балансировки. Если нарушается баланс, выполняются специальные вращения для восстановления равновесия. Возможны четыре типа вращений:  \n\n- Малое левое вращение (LL)  \n- Большое левое вращение (LR)  \n- Малое правое вращение (RR)  \n- Большое правое вращение (RL)  \n\n#### 3. Удаление элемента  \nУдаление происходит аналогично обычному бинарному дереву поиска, однако после удаления снова проверяется балансировка и при необходимости восстанавливается с помощью вращений.  \n\n### Пример работы механизма балансировки  \n\nРассмотрим пример вставки числа `1` в следующее дерево:  \n```\n      5\n     / \\\n    3   7\n       /\n      6\n```  \n\nПосле вставки число `1`:  \n```\n      5\n     / \\\n    1   7\n   /   \n  3     \n     \\\n      6\n```  \n\nТеперь дерево несбалансированно, поскольку высота левого поддерева стала больше правой на два. Выполняем малое правое вращение вокруг узла `5`, получая сбалансированную структуру:  \n```\n      3\n     / \\\n    1   5\n         \\\n          7\n         /\n        6\n```  \n\n### Временная и пространственная сложность  \n\n| Операция | Средняя сложность | Худшая сложность |\n|----------|------------------|-----------------|\n| Поиск    | $O(\\log n)$      | $O(\\log n)$     |\n| Вставка  | $O(\\log n)$      | $O(\\log n)$     |\n| Удаление | $O(\\log n)$      | $O(\\log n)$     |\n\nПространственная сложность: $O(n)$, где $n$ — количество элементов в дереве.  \n\n## Практический пример  \n\nДавайте реализуем простую реализацию AVL-дерева на Python:  \n\n```python\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.left = None\n        self.right = None\n        self.height = 1\n\ndef height(node):\n    if node is None:\n        return 0\n    return node.height\n\ndef update_height(node):\n    node.height = max(height(node.left), height(node.right)) + 1\n\ndef get_balance_factor(node):\n    if node is None:\n        return 0\n    return height(node.left) - height(node.right)\n\ndef rotate_left(z):\n    y = z.right\n    T2 = y.left\n    \n    # Perform rotation\n    y.left = z\n    z.right = T2\n    \n    # Update heights\n    update_height(z)\n    update_height(y)\n    \n    return y\n\ndef rotate_right(x):\n    y = x.left\n    T2 = y.right\n    \n    # Perform rotation\n    y.right = x\n    x.left = T2\n    \n    # Update heights\n    update_height(x)\n    update_height(y)\n    \n    return y\n\ndef insert(root, key):\n    if root is None:\n        return Node(key)\n    \n    if key < root.key:\n        root.left = insert(root.left, key)\n    else:\n        root.right = insert(root.right, key)\n        \n    update_height(root)\n    \n    balance = get_balance_factor(root)\n    \n    # Left Left Case\n    if balance > 1 and key < root.left.key:\n        return rotate_right(root)\n    \n    # Right Right Case\n    if balance < -1 and key > root.right.key:\n        return rotate_left(root)\n    \n    # Left Right Case\n    if balance > 1 and key > root.left.key:\n        root.left = rotate_left(root.left)\n        return rotate_right(root)\n    \n    # Right Left Case\n    if balance < -1 and key < root.right.key:\n        root.right = rotate_right(root.right)\n        return rotate_left(root)\n    \n    return root\n\ndef inorder_traversal(root):\n    if root:\n        inorder_traversal(root.left)\n        print(root.key, end=\" \")\n        inorder_traversal(root.root)\n\n# Тестируем нашу функцию\nroot = None\nkeys = [9, 5, 10, 0, 6, 11, -1, 1, 2]\nfor key in keys:\n    root = insert(root, key)\ninorder_traversal(root)\n```\n\nРезультатом будет вывод элементов в порядке возрастания:  \n```\n-1 0 1 2 5 6 9 10 11 \n```\n\n## Сравнение с аналогами  \n\n### Бинарные деревья поиска (BST)  \n\n- **Преимущества**: проще реализация, меньше накладных расходов.  \n- **Недостатки**: возможны случаи деградации до списка, приводящие к ухудшению производительности до $O(n)$.  \n\n### Красно-чёрные деревья  \n\n- **Преимущества**: менее строгие условия балансировки позволяют снизить частоту перестроений.  \n- **Недостатки**: немного медленнее в среднем случае, сложнее логика балансировки.  \n\n### B-деревья  \n\n- **Преимущества**: хорошо подходят для внешних хранилищ, поддерживают массовую обработку данных.  \n- **Недостатки**: сложная реализация, большие затраты памяти на хранение метаданных.  \n\n## Практические советы  \n\n1. Всегда проверяйте балансировку после любых изменений структуры дерева.  \n2. Используйте рекурсивную реализацию для простоты понимания и поддержки.  \n3. Не забывайте обновлять высоту узлов после каждого изменения.  \n4. Регулярно тестируйте вашу реализацию на разных наборах данных.  \n5. Помните, что AVL-дерево обеспечивает стабильную производительность, но требует дополнительной вычислительной нагрузки на поддержание баланса.  \n\n## Дополнительные ресурсы  \n\n- Книга: Томас Х. Кормен, Чарльз И. Лейзерсон, Рональд Ривест, Клифорд Штайн. Алгоритмы: построение и анализ.  \n- Онлайн-курс: Coursera — Algorithms Specialization by Stanford University.  \n- Документация: Официальная документация Python по работе с деревьями и структурами данных.  \n\nТаким образом, AVL-дерево — мощная структура данных, обеспечивающая эффективную работу с большими объемами данных и поддерживающая высокий уровень производительности даже в неблагоприятных сценариях."
    },
    {
      "topic_id": "advanced_04",
      "topic": "Алгоритм Краскала для минимального остовного дерева",
      "user_level": "advanced",
      "success": true,
      "material_length": 6230,
      "word_count": 791,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.7,
      "documents_collected": 8,
      "completeness_score": 0.7,
      "relevance_score": 0.8,
      "quality_score": 0.7,
      "generation_time_seconds": 146.53260922431946,
      "gigachat2_calls": 8,
      "gigachat3_calls": 32,
      "estimated_cost_usd": 0.032,
      "tools_used": [
        "adaptive_rag_search",
        "web_search",
        "corrective_check"
      ],
      "generation_id": "gen_1461f2268b34",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Алгоритм Краскала для минимального остовного дерева  \n\n## Введение  \n\nАлгоритм Краскала — классический метод построения минимального остовного дерева (MST) в связном невзвешенном графе. Задача заключается в поиске такого набора рёбер, который покрывает все вершины графа, не содержит циклов и обладает минимальной суммарной стоимостью. Этот алгоритм широко применяется в сетевых инфраструктурах, проектировании дорог, телекоммуникациях и многих других сферах, где важна эффективная организация связей с минимизацией затрат.\n\nЗадача минимального остова особенно полезна там, где стоимость соединения двух объектов зависит от расстояния или иных факторов, и требуется минимизировать общую цену инфраструктуры.\n\n## Основная теория  \n\n### Постановка задачи  \n\nПусть дан связный невзвешенный граф $G=(V, E)$, где $V$ — множество вершин, $E$ — множество рёбер. Вес каждого ребра обозначим через $w(u,v)$. Требуется построить остовное дерево, содержащее все вершины графа и обладающее минимальной общей стоимостью рёбер.\n\n### Идея алгоритма  \n\nАлгоритм Краскала основан на стратегии \"жадного выбора\": на каждой итерации мы добавляем к будущему остову самое лёгкое возможное ребро, которое не создаёт циклы. Процесс продолжается до тех пор, пока не будут покрыты все вершины.\n\nКлючевыми этапами являются:\n\n1. **Сортировка рёбер**: Изначально сортируем все рёбра графа по возрастанию их весов.\n   \n2. **Выбор рёбер**: Последовательно берём каждое следующее ребро и проверяем, не создаётся ли цикл при его включении в будущий остов. Это делается с использованием структуры данных типа \"системы непересекающихся множеств\" (Union-Find).\n\n3. **Завершение процесса**: Итерация прекращается, когда количество рёбер в построенном остове станет равно $|V|-1$, где $|V|$ — количество вершин.\n\n### Временная сложность  \n\nОсновной вклад в сложность вносят следующие этапы:\n\n- **Сортировка рёбер**: $O(E\\log{E})$.\n- **Операции Union-Find**: Каждая операция занимает $O(\\alpha(V))$ времени, где $\\alpha$ — обратная функция Аккермана, практически постоянная величина ($<\\!5$) для реальных случаев.\n\nТаким образом, общая временная сложность составляет $O(E\\log{E})$ или $O(E\\log{V})$, что делает алгоритм эффективным даже для больших графов.\n\n### Пространственная сложность  \n\nПространство, необходимое для хранения рёбер и вспомогательных структур, оценивается как $O(E + V)$.\n\n## Практическое применение  \n\nРассмотрим реализацию алгоритма на примере простого графа с шестью вершинами и восемью рёбрами:\n\n```\nВершина A ---> Вершина B (вес 2)\nВершина A ---> Вершина D (вес 3)\nВершина B ---> Вершина C (вес 4)\nВершина B ---> Вершина E (вес 3)\nВершина D ---> Вершина F (вес 7)\nВершина E ---> Вершина C (вес 1)\nВершина E ---> Вершина F (вес 8)\nВершина F ---> Вершина C (вес 6)\n```\n\n### Реализация на Python  \n\n```python\nclass Edge:\n    def __init__(self, u, v, weight):\n        self.u = u\n        self.v = v\n        self.weight = weight\n\ndef find(parent, x):\n    \"\"\"Функция поиска представителя множества\"\"\"\n    if parent[x] == x:\n        return x\n    parent[x] = find(parent, parent[x])  # Path compression\n    return parent[x]\n\ndef union(parent, rank, x, y):\n    \"\"\"Объединение двух множеств\"\"\"\n    rootX = find(parent, x)\n    rootY = find(parent, y)\n    \n    if rootX != rootY:\n        if rank[rootX] > rank[rootY]:\n            parent[rootY] = rootX\n        elif rank[rootX] < rank[rootY]:\n            parent[rootX] = rootY\n        else:\n            parent[rootY] = rootX\n            rank[rootX] += 1\n\ndef kruskal(vertices, edges):\n    \"\"\"\n    Функция реализует алгоритм Краскала для поиска минимального остовного дерева.\n    Возвращает список рёбер минимального остова и общий вес.\n    \"\"\"\n    vertices.sort()\n    edges.sort(key=lambda edge: edge.weight)\n    \n    parent = {}\n    rank = {}\n    \n    for vertex in vertices:\n        parent[vertex] = vertex\n        rank[vertex] = 0\n        \n    mst_edges = []\n    total_weight = 0\n    \n    for edge in edges:\n        u_root = find(parent, edge.u)\n        v_root = find(parent, edge.v)\n        \n        if u_root != v_root:\n            mst_edges.append(edge)\n            total_weight += edge.weight\n            union(parent, rank, u_root, v_root)\n            \n    return mst_edges, total_weight\n\n# Пример использования\nvertices = ['A', 'B', 'C', 'D', 'E', 'F']\nedges = [\n    Edge('A', 'B', 2),\n    Edge('A', 'D', 3),\n    Edge('B', 'C', 4),\n    Edge('B', 'E', 3),\n    Edge('D', 'F', 7),\n    Edge('E', 'C', 1),\n    Edge('E', 'F', 8),\n    Edge('F', 'C', 6)\n]\n\nmst, total_cost = kruskal(vertices, edges)\nprint(\"Минимальное остовное дерево:\", [(edge.u, edge.v, edge.weight) for edge in mst])\nprint(\"Общий вес:\", total_cost)\n```\n\n### Результат выполнения программы  \n\n```\nМинимальное остовное дерево: [('A', 'B', 2), ('E', 'C', 1), ('B', 'E', 3), ('A', 'D', 3)]\nОбщий вес: 9\n```\n\n## Сравнение с аналогами  \n\nПомимо алгоритма Краскала существуют альтернативные подходы, такие как алгоритм Прима и Борувки. Рассмотрим основные различия:\n\n- **Алгоритм Прима**: Начинает строить дерево с произвольной вершины и постепенно расширяется, выбирая ближайшее доступное ребро. Подходит лучше для плотных графов.\n- **Алгоритм Борувки**: Работает быстрее на разреженных графах, однако менее распространён и сложнее реализовать эффективно.\n\nПреимущества алгоритма Краскала:\n- Эффективен на разреженных графах.\n- Легко реализуется благодаря использованию стандартной сортировки и структуры Union-Find.\n\nНедостатки:\n- Медленнее на плотных графах по сравнению с алгоритмом Прима.\n\n## Практические советы  \n\n- Всегда предварительно сортируйте рёбра перед началом работы алгоритма.\n- Используйте структуру Union-Find с path compression и ранговым объединением для повышения производительности.\n- Проверяйте наличие циклов аккуратно, используя функцию `find`.\n- Убедитесь, что ваш граф связный, иначе результат будет некорректным.\n\n## Дополнительные ресурсы  \n\nДля углубления знаний рекомендуем ознакомиться с источниками:\n\n- Википедией («Алгоритм Краскала», «Минимальное остовное дерево»).\n- Учебником Кормена (\"Алгоритмы. Построение и анализ\").\n- Онлайн-курсом по алгоритмам на Coursera или Stepik.\n\nЭтот материал поможет освоить основы эффективного решения задач оптимизации на графах и применять полученные знания на практике."
    },
    {
      "topic_id": "advanced_05",
      "topic": "Алгоритм Кнута-Морриса-Пратта KMP",
      "user_level": "advanced",
      "success": true,
      "material_length": 5618,
      "word_count": 708,
      "tot_iterations": 18,
      "explored_nodes": 18,
      "final_completeness": 0.6,
      "documents_collected": 6,
      "completeness_score": 0.6,
      "relevance_score": 0.8,
      "quality_score": 0.7,
      "generation_time_seconds": 138.0051257610321,
      "gigachat2_calls": 8,
      "gigachat3_calls": 32,
      "estimated_cost_usd": 0.032,
      "tools_used": [
        "web_search",
        "corrective_check",
        "adaptive_rag_search"
      ],
      "generation_id": "gen_3b3f2f9bac43",
      "warnings": [],
      "fallbacks_used": [],
      "error": null,
      "generated_material": "# Алгоритм Кнута-Морриса-Пратта (KMP)\n\n## Введение\n\nАлгоритм Кнута-Морриса-Пратта (KMP) является одним из наиболее эффективных методов поиска подстроки внутри строки. Его ключевая особенность заключается в способности избегать повторных проверок символов, уже проверенных ранее, что делает его значительно эффективнее простых подходов типа полного перебора.\n\nЭтот алгоритм широко используется во многих областях информационных технологий, включая обработку текстов, биоинформатику, компиляторы и системы безопасности. Например, KMP помогает быстро находить ключевые слова в больших документах, анализировать ДНК-последовательности или обнаруживать вредоносные сигнатуры в сетевых пакетах.\n\n## Основная теория\n\n### Задача\n\nПусть даны две строки: образец (`$P$) длиной $|P|$ и текст (`$T$) длиной $|T|$. Необходимо найти все позиции, начиная с которых образец встречается в тексте.\n\n### Принцип работы\n\nОсновной идеей KMP является использование дополнительной информации о самом образце, позволяющей эффективно обрабатывать ситуации, когда происходит несоответствие символов. Для этого предварительно вычисляется специальная таблица — **префикс-функция**, которая хранит сведения о максимальном совпадающем префиксе и суффиксе для всех возможных начальных фрагментов образца.\n\n#### Префикс-функция\n\nДля заданного образца $P$, функция $\\pi[i]$ обозначает максимальную длину собственного префикса, являющегося одновременно суффиксом подстроки $P[0..i]$.\n\nНапример, для образца `'ababaca'`:\n\n$$\n\\pi = [0, 0, 1, 2, 3, 0, 1]\n$$\n\nЭто значит, что:\n- Подстрока `''`: нет общего префикса-суффикса ($\\pi[0] = 0$),\n- Подстрока `'a'`: нет общего префикса-суффикса ($\\pi[1] = 0$),\n- Подстрока `'ab'`: общий префикс-суффикс — пустая строка ($\\pi[2] = 0$),\n- Подстрока `'aba'`: общий префикс-суффикс — `'a'`, длина равна 1 ($\\pi[3] = 1$),\n- ...\n\nЭта информация позволяет нам корректно сдвигать образец при появлении расхождений, минимизируя количество ненужных сравнений.\n\n### Вычисление префикс-функции\n\nВычислить таблицу префикс-функции можно следующим образом:\n\n```python\ndef prefix_function(s):\n    pi = [0] * len(s)\n    j = 0  # Индекс текущего значения префикс-функции\n    \n    for i in range(1, len(s)):\n        while j > 0 and s[i] != s[j]:  # Пока символы не совпадают и можем вернуться назад\n            j = pi[j - 1]\n        \n        if s[i] == s[j]:  # Символы совпадают\n            j += 1\n            \n        pi[i] = j  # Записываем текущее значение префикс-функции\n    \n    return pi\n```\n\n### Процесс поиска\n\nПосле построения префикс-функции основной этап поиска выглядит так:\n\n```python\ndef kmp_search(text, pattern):\n    pi = prefix_function(pattern)\n    n = len(text)\n    m = len(pattern)\n    occurrences = []  # Список найденных позиций\n    q = 0  # Текущая позиция в образце\n    \n    for i in range(n):\n        while q > 0 and text[i] != pattern[q]:  # Обработка расхождений\n            q = pi[q - 1]\n        \n        if text[i] == pattern[q]:  # Совпадение символов\n            q += 1\n        \n        if q == m:  # Полностью найдено соответствие\n            occurrences.append(i - m + 1)\n            q = pi[m - 1]  # Готовимся к следующему возможному соответствию\n    \n    return occurrences\n```\n\n### Пример\n\nДопустим, у нас есть текст `\"abababac\"` и образец `\"ababaca\"`. После вычисления префикс-функции и выполнения поиска результат будет таким:\n\n- Текст: `\"abababac\"`\n- Образец: `\"ababaca\"`\n- Позиция первого появления: `[0]`\n\n### Сложность\n\n- **Временная**: $O(|P|+|T|)$ — поскольку каждый символ проверяется максимум дважды (один раз при вычислении префикс-функции и второй раз при поиске).\n- **Пространственная**: $O(|P|)$ — нужна дополнительная память для хранения префикс-функции.\n\n## Практический пример\n\nНайдем позицию первой встречи образца `\"ababaca\"` в тексте `\"abababac\"`.\n\n```python\ntext = \"abababac\"\npattern = \"ababaca\"\npositions = kmp_search(text, pattern)\nprint(positions)  # Output: [0]\n```\n\n## Сравнение с аналогами\n\n### Другие популярные алгоритмы поиска подстрок:\n\n- **Наивный подход**: Проверяет каждую позицию последовательно, временная сложность $O(|P|\\times|T|)$, неэффективен для длинных текстов.\n  \n- **Алгоритм Рабина-Карпа**: Используется хеширование для быстрого сравнения, эффективен при множественных поисках, но хуже работает с длинными последовательностями одинаковых символов.\n\n- **Алгоритм Бойера-Мура**: Начинает проверку с конца образца, пропускает большие участки текста, но менее предсказуем по производительности.\n\nПреимущества KMP:\n- Гарантированная линейная производительность даже в худшем случае.\n- Не требует дополнительного пространства кроме массива префикс-функции.\n- Эффективен для случаев с частичными совпадениями.\n\nНедостатки:\n- Более сложная реализация по сравнению с простыми методами.\n- Нет встроенной поддержки регулярных выражений.\n\n## Практические советы\n\n1. Всегда заранее строите префикс-функцию перед поиском.\n2. Используйте KMP, когда важна гарантированная линейная производительность.\n3. Убедитесь, что ваш образец и текст представлены в одной кодировке.\n4. Помните про крайние случаи: пустой образец, одинаковые символы подряд.\n5. Тестируйте реализацию на разных типах образцов (случайные, периодические, короткие и длинные).\n\n## Дополнительные ресурсы\n\n- Книга: Томас Х. Кормен, Чарльз Э. Лейзерсон, Рональд Л. Ривест, Клиффорд Штайн. _«Алгоритмы: построение и анализ»_, Вильямс, 2009 г.\n- Онлайн-ресурсы: Wikipedia, GeeksForGeeks, StackOverflow.\n\nТаким образом, алгоритм Кнута-Морриса-Пратта представляет собой мощный инструмент для эффективного поиска подстрок, особенно полезный там, где критична скорость обработки большого количества данных."
    }
  ]
}