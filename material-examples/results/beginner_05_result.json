{
  "topic_id": "beginner_05",
  "topic": "Сложность алгоритмов Big O",
  "user_level": "beginner",
  "success": true,
  "material_length": 4803,
  "word_count": 615,
  "tot_iterations": 18,
  "explored_nodes": 18,
  "final_completeness": 0.6,
  "documents_collected": 6,
  "completeness_score": 0.6,
  "relevance_score": 1.0,
  "quality_score": 0.8,
  "generation_time_seconds": 162.77947330474854,
  "gigachat2_calls": 9,
  "gigachat3_calls": 36,
  "estimated_cost_usd": 0.036,
  "tools_used": [
    "web_search",
    "corrective_check",
    "adaptive_rag_search"
  ],
  "generation_id": "gen_9273f74ac8d4",
  "warnings": [],
  "fallbacks_used": [],
  "error": null,
  "generated_material": "# Учебный материал  \n## Сложность алгоритмов Big O  \n\n### Уровень подготовки: Beginner  \n\n#### Введение  \nКаждый день разработчики сталкиваются с необходимостью решать задачи, связанные с обработкой данных разного объёма. Чем больше объём данных, тем важнее становится понимать, насколько эффективно работает тот или иной алгоритм. Именно здесь вступает в игру концепция **сложности алгоритмов**, выраженная через нотацию **Big O**. Эта нотация позволяет сравнить разные алгоритмы и выбрать оптимальный вариант для конкретной ситуации. \n\nПредставьте себе ситуацию: вы работаете над проектом интернет-магазина, где пользователи ищут товары среди тысяч позиций. Важно, чтобы поиск был быстрым и удобным. Выбор правильного алгоритма поиска позволит сделать ваш магазин привлекательным для покупателей, увеличивая продажи и лояльность аудитории. Вот тут и пригодится понимание сложности алгоритмов.  \n\n#### Основная теория  \n\n##### Что такое Big O?  \n**Big O** — это специальный инструмент для описания скорости работы алгоритма относительно количества обрабатываемых данных. Проще говоря, она показывает, как изменится производительность программы, если увеличится объём данных.  \n\nНапример, если у вас есть список из 100 элементов, и вы хотите проверить наличие какого-либо элемента, самый простой способ — просмотреть весь список последовательно. Но если элементов станет миллион, такая проверка займёт значительно больше времени. Здесь и приходит на помощь Big O, помогая заранее предсказать, сколько ресурсов потребуется вашему алгоритму.  \n\n##### Основные типы сложности  \nВот основные виды сложности, которые встречаются чаще всего:  \n\n- **O(1)** — постоянная сложность. Независимо от размера данных, выполнение занимает одинаковое время. Например, получение элемента массива по индексу.  \n- **O(log N)** — логарифмическая сложность. Скорость растёт гораздо медленнее, чем сам объём данных. Хороший пример — бинарный поиск.  \n- **O(N)** — линейная сложность. Время выполнения прямо пропорционально объёму данных. Например, последовательный поиск в списке.  \n- **O(N²)** — квадратичная сложность. Каждый дополнительный элемент увеличивает время выполнения экспоненциально. Обычно возникает при наличии вложенных циклов.  \n- **O(N³)** — кубическая сложность. Ещё хуже, чем квадратичная. Возникает редко, но иногда бывает в трёхмерных задачах.  \n- **O(N!)** — факториальная сложность. Самый плохой сценарий, когда каждая операция требует полного перебора всех предыдущих шагов. Используется крайне редко.  \n\n##### Зачем нужна оценка сложности?  \nОценивая сложность своего алгоритма, вы можете:  \n\n- Предсказать, как поведёт себя программа при росте объёма данных.  \n- Определить узкие места и оптимизировать их.  \n- Подобрать подходящий алгоритм для вашей задачи.  \n\n#### Практическое применение  \n\nДопустим, у вас есть два списка чисел длиной `n`. Вам нужно найти общие элементы обоих списков. Рассмотрим два подхода:  \n\n```python\n# Подход №1: О(N²)\ndef find_common_elements(list1, list2):\n    common = []\n    for num in list1:       # O(N)\n        if num in list2:    # O(N)\n            common.append(num)\n    return common           # Общая сложность: O(N*N) = O(N²)\n```\n\nЭтот метод имеет квадратичную сложность, так как для каждой позиции первого списка проверяется вся длина второго списка.  \n\nТеперь посмотрим другой подход:  \n\n```python\n# Подход №2: О(N + M)\nfrom collections import Counter\n\ndef find_common_elements_optimized(list1, list2):\n    counter_list1 = Counter(list1)      # O(N)\n    result = [num for num in list2 if counter_list1[num]]  # O(M)\n    return result                       # Общая сложность: O(N+M)\n```\n\nВторой подход эффективнее, так как использует структуру данных (`Counter`), позволяющую проверять наличие элемента за постоянное время.  \n\n#### Практические советы  \n\n- Всегда оценивайте сложность ваших решений перед реализацией.  \n- Старайтесь избегать вложенных циклов там, где это возможно.  \n- Используйте подходящие структуры данных для ускорения поиска и вставки.  \n- Помните, что оптимизация важна, но чрезмерная преждевременная оптимизация тоже может привести к усложнению кода.  \n\n#### Заключение  \nЗнание сложности алгоритмов — важный навык для любого разработчика. Оно позволяет создавать быстрые и производительные приложения, способные справляться с большими нагрузками. Изучив эту концепцию, вы сможете уверенно подходить к решению любых задач, связанных с обработкой данных.  \n\n#### Дополнительные материалы  \n- Книга: Томас Х. Кормен, Чарльз И. Лейзерсон, Рональд Л. Ривест, Клифффорд Штайн — «Алгоритмы: построение и анализ».  \n- Онлайн-курс: Coursera — «Algorithms Specialization by Stanford University».  \n\nНадеюсь, этот материал оказался полезным и дал вам хорошее представление о важности понимания сложности алгоритмов. Удачного освоения материала!"
}