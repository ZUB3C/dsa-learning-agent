"""
Adaptive RAG Tool: Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.
Code from Section 5.1 of architecture.
"""

import asyncio
import logging
import operator
import time
from typing import Any

from src.config import get_settings
from src.core.llm import get_llm
from src.core.vector_store import vector_store_manager
from src.exceptions import ChromaDBUnavailableError
from src.tools.base_tool import BaseTool, Document, ToolResult

logger = logging.getLogger(__name__)


class AdaptiveRAGTool(BaseTool):
    """
    Adaptive RAG: Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.

    Strategies:
    - TF-IDF: Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾, keyword-based)
    - Semantic: Ğ´Ğ»Ñ ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (ChromaDB embeddings)
    - Hybrid: Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (RRF fusion)

    Uses: GigaChat3 Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
    """

    name = "adaptive_rag_search"
    description = """
    ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸.

    Params:
      query (str): Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
      strategy (str): "auto" | "tfidf" | "semantic" | "hybrid"
      k (int): ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (default: 5)

    Returns:
      ToolResult with documents
    """

    def __init__(self) -> None:
        super().__init__()
        self.settings = get_settings()
        self.vector_store = vector_store_manager
        self.tfidf_retriever = None  # Lazy init
        self.llm_classifier = get_llm(use_gigachat3=True)

    async def execute(self, params: dict[str, Any]) -> ToolResult:
        """Execute adaptive RAG search."""
        query = params.get("query", "")
        strategy = params.get("strategy", "auto")
        k = params.get("k", self.settings.adaptive_rag.rag_top_k)

        if not query:
            return ToolResult(success=False, documents=[], error="Query parameter is required")

        start_time = time.time()

        logger.info(f"ğŸ” Adaptive RAG: query='{query[:50]}...', strategy={strategy}, k={k}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 1: STRATEGY SELECTION
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        if strategy == "auto":
            strategy = await self._classify_query_complexity(query)
            logger.info(f"ğŸ” Auto-selected strategy: {strategy}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 2: RETRIEVAL
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        try:
            if strategy == "tfidf":
                documents = await self._tfidf_search(query, k)
            elif strategy == "semantic":
                documents = await self._semantic_search(query, k)
            elif strategy == "hybrid":
                documents = await self._hybrid_search(query, k)
            else:
                raise ValueError(f"Unknown strategy: {strategy}")

            logger.info(f"âœ… Retrieved {len(documents)} documents via {strategy}")

        except Exception as e:
            logger.exception(f"âŒ Retrieval failed: {e}")

            # FALLBACK CHAIN
            if strategy != "semantic":
                logger.warning("âš ï¸ Falling back to semantic search")
                try:
                    documents = await self._semantic_search(query, k)
                except Exception as e2:
                    logger.exception(f"âŒ Semantic fallback failed: {e2}")
                    return ToolResult(
                        success=False,
                        documents=[],
                        error=str(e2),
                        metadata={"strategy_attempted": strategy},
                    )
            else:
                return ToolResult(
                    success=False,
                    documents=[],
                    error=str(e),
                    metadata={"strategy_attempted": strategy},
                )

        execution_time = (time.time() - start_time) * 1000  # ms

        return ToolResult(
            success=len(documents) > 0,
            documents=documents,
            metadata={
                "strategy_used": strategy,
                "count": len(documents),
                "execution_time_ms": execution_time,
            },
            execution_time_ms=execution_time,
        )

    async def _classify_query_complexity(self, query: str) -> str:
        """
        Classify query complexity to select retrieval strategy.

        Uses: Rule-based (fast) + GigaChat3 fallback (accurate)
        """

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # METHOD 1: RULE-BASED (Fast, deterministic)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        query_length = len(query)
        word_count = len(query.split())

        # Technical terms that indicate complexity
        complex_indicators = [
            "ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ",
            "Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·",
            "Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ",
            "Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ",
            "Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸",
            "ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ",
            "vs",
            "Ğ¸Ğ»Ğ¸",
            "Ğ»ÑƒÑ‡ÑˆĞµ",
        ]

        has_complex_indicator = any(ind in query.lower() for ind in complex_indicators)

        # Decision logic
        if query_length < self.settings.adaptive_rag.adaptive_simple_threshold and word_count <= 3:
            return "tfidf"  # Simple: "Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°"
        if (
            query_length > self.settings.adaptive_rag.adaptive_complex_threshold
            or has_complex_indicator
        ):
            return "hybrid"  # Complex: "Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸..."
        return "semantic"  # Medium: "ĞšĞ°Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ”ĞµĞ¹ĞºÑÑ‚Ñ€Ñ‹?"

    async def _tfidf_search(self, query: str, k: int) -> list[Document]:
        """
        TF-IDF based search (keyword matching).
        Fast, good for simple queries.

        Fallback: Rebuild TF-IDF if model not found
        """

        # TODO: Implement TF-IDF retriever
        # For now, fallback to semantic
        logger.warning("âš ï¸ TF-IDF not implemented, falling back to semantic")
        return await self._semantic_search(query, k)

    async def _semantic_search(self, query: str, k: int) -> list[Document]:
        """
        Semantic search via ChromaDB embeddings.
        Best for natural language queries.

        Fallback: Retry with increased timeout
        """

        try:
            results = self.vector_store.similarity_search(query=query, k=k)

            # Convert to Document objects
            return [
                Document(
                    page_content=doc.page_content,
                    metadata=doc.metadata,
                    source=doc.metadata.get("source", ""),
                    relevance_score=1.0,  # ChromaDB doesn't return scores by default
                )
                for doc in results
            ]

        except TimeoutError:
            logger.warning("âš ï¸ ChromaDB timeout, retrying with extended timeout...")

            # Retry with 2x timeout
            try:
                results = self.vector_store.similarity_search(query=query, k=k)

                return [
                    Document(
                        page_content=doc.page_content,
                        metadata=doc.metadata,
                        source=doc.metadata.get("source", ""),
                        relevance_score=1.0,
                    )
                    for doc in results
                ]

            except Exception as e:
                logger.exception(f"âŒ Semantic search retry failed: {e}")
                raise ChromaDBUnavailableError(str(e))

        except Exception as e:
            logger.exception(f"âŒ Semantic search failed: {e}")
            raise

    async def _hybrid_search(self, query: str, k: int) -> list[Document]:
        """
        Hybrid: TF-IDF + Semantic with Reciprocal Rank Fusion.
        Best for complex queries.

        Fallback: Use whichever method succeeds
        """

        # Run both in parallel
        tasks = [
            self._tfidf_search(query, k=k * 2),  # Get more for fusion
            self._semantic_search(query, k=k * 2),
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        tfidf_docs = results[0] if not isinstance(results[0], Exception) else []
        semantic_docs = results[1] if not isinstance(results[1], Exception) else []

        # FALLBACK: If one failed, use the other
        if not tfidf_docs and semantic_docs:
            logger.warning("âš ï¸ TF-IDF failed, using semantic only")
            return semantic_docs[:k]
        if not semantic_docs and tfidf_docs:
            logger.warning("âš ï¸ Semantic failed, using TF-IDF only")
            return tfidf_docs[:k]
        if not tfidf_docs and not semantic_docs:
            logger.error("âŒ Both retrievers failed")
            return []

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # RECIPROCAL RANK FUSION (RRF)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        k_constant = self.settings.adaptive_rag.rrf_k_constant
        doc_scores = {}  # {doc_id: score}
        doc_objects = {}  # {doc_id: Document}

        # Score from TF-IDF
        for rank, doc in enumerate(tfidf_docs, start=1):
            doc_id = doc.metadata.get("id", doc.page_content[:50])
            score = 1 / (k_constant + rank)
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + score
            doc_objects[doc_id] = doc

        # Score from Semantic
        for rank, doc in enumerate(semantic_docs, start=1):
            doc_id = doc.metadata.get("id", doc.page_content[:50])
            score = 1 / (k_constant + rank)
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + score
            if doc_id not in doc_objects:
                doc_objects[doc_id] = doc

        # Sort by RRF score
        sorted_doc_ids = sorted(doc_scores.items(), key=operator.itemgetter(1), reverse=True)

        # Return top-k
        fused_docs = [doc_objects[doc_id] for doc_id, score in sorted_doc_ids[:k]]

        logger.info(
            f"âœ… Hybrid RRF fusion: {len(tfidf_docs)} + {len(semantic_docs)} â†’ {len(fused_docs)}"
        )

        return fused_docs
