"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—Ç–æ—Ä–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤ –≤—Ç–æ—Ä—ã–º LLM.
–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown.
"""

import argparse
import asyncio
import json
import sys
from datetime import datetime
from enum import StrEnum
from pathlib import Path

from pydantic import BaseModel, Field

from src.agents.registry import load_agent


class DifficultyLevel(StrEnum):
    """–£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞"""

    EASY = "easy"
    MEDIUM = "medium"
    HARD = "hard"


class Question(BaseModel):
    question_id: int = Field(..., description="ID –≤–æ–ø—Ä–æ—Å–∞")
    difficulty: DifficultyLevel = Field(..., description="–°–ª–æ–∂–Ω–æ—Å—Ç—å")
    question_text: str = Field(..., description="–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞")
    expected_answer: str = Field(..., description="–≠—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç")
    user_answer: str
    key_points: list[str] = Field(..., description="–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã")


class Topic(BaseModel):
    topic_id: str = Field(..., description="ID —Ç–µ–º—ã")
    topic_name: str = Field(..., description="–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã")
    questions: list[Question] = Field(..., description="–í–æ–ø—Ä–æ—Å—ã")


class TestCollection(BaseModel):
    creation_date: str
    total_questions: int
    topics_count: int
    topics: list[Topic]


class PrimaryEvaluation(BaseModel):
    score: float = Field(..., ge=0, le=100)
    is_correct: bool
    feedback: str


class SecondaryEvaluation(BaseModel):
    agree_with_primary: bool
    final_score: float = Field(..., ge=0, le=100)
    final_feedback: str
    verification_notes: str | None = None


class TestVerification(BaseModel):
    question_id: int
    topic: str
    difficulty: DifficultyLevel
    primary_evaluation: PrimaryEvaluation
    secondary_evaluation: SecondaryEvaluation
    timestamp: str


class VerificationMetrics(BaseModel):
    total_verifications: int
    agreement_count: int
    disagreement_count: int
    agreement_rate: float = Field(..., ge=0, le=100)
    average_score_difference: float
    correctness_match_rate: float = Field(..., ge=0, le=100)


class DifficultyMetrics(BaseModel):
    difficulty: DifficultyLevel
    metrics: VerificationMetrics


class TopicMetrics(BaseModel):
    topic: str
    metrics: VerificationMetrics


class EffectivenessReport(BaseModel):
    report_date: str
    overall_metrics: VerificationMetrics
    by_difficulty: list[DifficultyMetrics]
    by_topic: list[TopicMetrics]
    verifications: list[TestVerification]


def metrics_to_markdown_table(metrics: VerificationMetrics) -> str:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ markdown —Ç–∞–±–ª–∏—Ü—É"""
    return f"""
| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ |
|---------|----------|
| –í—Å–µ–≥–æ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–π | {metrics.total_verifications} |
| –°–æ–≥–ª–∞—Å–∏—è | {metrics.agreement_count} ({metrics.agreement_rate:.1f}%) |
| –ù–µ—Å–æ–≥–ª–∞—Å–∏—è | {metrics.disagreement_count} ({100 - metrics.agreement_rate:.1f}%) |
| –°—Ä–µ–¥–Ω—è—è —Ä–∞–∑–Ω–∏—Ü–∞ –æ—Ü–µ–Ω–æ–∫ | {metrics.average_score_difference:.2f} –ø—É–Ω–∫—Ç–æ–≤ |
| –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ | {metrics.correctness_match_rate:.1f}% |
"""


def generate_markdown_report(report: EffectivenessReport) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown"""

    md_lines = []

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    md_lines.append("# üìä –û—Ç—á–µ—Ç –æ–± —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—Ç–æ—Ä–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ LLM")
    md_lines.append("")
    md_lines.append(f"**–î–∞—Ç–∞ –æ—Ç—á–µ—Ç–∞:** {report.report_date}")
    md_lines.append("")

    # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    md_lines.append("## –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏")
    md_lines.append("")
    md_lines.append(metrics_to_markdown_table(report.overall_metrics))
    md_lines.append("")

    # –ü–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    md_lines.append("## –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —É—Ä–æ–≤–Ω—é —Å–ª–æ–∂–Ω–æ—Å—Ç–∏")
    md_lines.append("")
    md_lines.append("| –°–ª–æ–∂–Ω–æ—Å—Ç—å | –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–π | –°–æ–≥–ª–∞—Å–∏–µ | –°—Ä–µ–¥–Ω—è—è —Ä–∞–∑–Ω–∏—Ü–∞ |")
    md_lines.append("|-----------|-------------|---------|-----------------|")
    md_lines.extend(
        f"| {diff_metric.difficulty.value.upper()} | "
        f"{diff_metric.metrics.total_verifications} | "
        f"{diff_metric.metrics.agreement_rate:.1f}% | "
        f"{diff_metric.metrics.average_score_difference:.2f} |"
        for diff_metric in report.by_difficulty
    )
    md_lines.append("")

    # –ü–æ —Ç–µ–º–∞–º
    md_lines.append("## –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–µ–º–∞–º")
    md_lines.append("")
    md_lines.append("| –¢–µ–º–∞ | –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–π | –°–æ–≥–ª–∞—Å–∏–µ | –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ |")
    md_lines.append("|------|-------------|---------|-------------------------|")
    md_lines.extend(
        f"| {topic_metric.topic} | "
        f"{topic_metric.metrics.total_verifications} | "
        f"{topic_metric.metrics.agreement_rate:.1f}% | "
        f"{topic_metric.metrics.correctness_match_rate:.1f}% |"
        for topic_metric in report.by_topic
    )
    md_lines.append("")

    # –í—ã–≤–æ–¥—ã
    md_lines.append("## üí° –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
    md_lines.append("")

    # –£—Ä–æ–≤–µ–Ω—å —Å–æ–≥–ª–∞—Å–∏—è
    if report.overall_metrics.agreement_rate >= 85:
        md_lines.append("‚úÖ **–í—Ç–æ—Ä–∏—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –í–´–°–û–ö–û–≠–§–§–ï–ö–¢–ò–í–ù–ê** (>= 85% —Å–æ–≥–ª–∞—Å–∏—è)")
    elif report.overall_metrics.agreement_rate >= 70:
        md_lines.append("‚ö†Ô∏è **–í—Ç–æ—Ä–∏—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –•–û–†–û–®–û —Ä–∞–±–æ—Ç–∞–µ—Ç** (70-84% —Å–æ–≥–ª–∞—Å–∏—è)")
    elif report.overall_metrics.agreement_rate >= 50:
        md_lines.append("‚ö†Ô∏è **–í—Ç–æ—Ä–∏—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –£–ú–ï–†–ï–ù–ù–ê** (50-69% —Å–æ–≥–ª–∞—Å–∏—è)")
    else:
        md_lines.append("‚ùå **–í—Ç–æ—Ä–∏—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ù–ï –°–û–ì–õ–ê–°–û–í–ê–ù–ê** (< 50% —Å–æ–≥–ª–∞—Å–∏—è)")
    md_lines.append("")

    # –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–æ–∫
    if report.overall_metrics.average_score_difference <= 3:
        md_lines.append("‚úÖ **–û—Ü–µ–Ω–∫–∏ –ö–û–ù–°–ò–°–¢–ï–ù–¢–ù–´** –º–µ–∂–¥—É –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏ (—Ä–∞–∑–Ω–∏—Ü–∞ <= 3)")
    elif report.overall_metrics.average_score_difference <= 10:
        md_lines.append("‚ö†Ô∏è **–û—Ü–µ–Ω–∫–∏ –∏–º–µ—é—Ç –î–û–ü–£–°–¢–ò–ú–´–ï —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è** (3-10)")
    else:
        md_lines.append("‚ùå **–û—Ü–µ–Ω–∫–∏ –∏–º–µ—é—Ç –ó–ù–ê–ß–ò–¢–ï–õ–¨–ù–´–ï —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è** (> 10)")
    md_lines.append("")

    # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏
    if report.overall_metrics.correctness_match_rate >= 90:
        md_lines.append("‚úÖ **–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –û–ß–ï–ù–¨ –í–´–°–û–ö–û–ï** (>= 90%)")
    elif report.overall_metrics.correctness_match_rate >= 80:
        md_lines.append("‚úÖ **–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –í–´–°–û–ö–û–ï** (80-89%)")
    else:
        md_lines.append("‚ö†Ô∏è **–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è** (< 80%)")
    md_lines.append("")

    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
    md_lines.append("## üìã –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–π")
    md_lines.append("")
    md_lines.append("| Q ID | –¢–µ–º–∞ | –°–ª–æ–∂–Ω–æ—Å—Ç—å | –ü–µ—Ä–≤–∏—á–Ω–∞—è | –í—Ç–æ—Ä–∏—á–Ω–∞—è | –°–æ–≥–ª–∞—Å–∏–µ |")
    md_lines.append("|------|------|-----------|-----------|-----------|----------|")

    for verif in report.verifications[:20]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 20
        agreement_mark = "‚úÖ" if verif.secondary_evaluation.agree_with_primary else "‚ùå"
        md_lines.append(
            f"| {verif.question_id} | {verif.topic} | {verif.difficulty.value} | "
            f"{verif.primary_evaluation.score:.0f} | "
            f"{verif.secondary_evaluation.final_score:.0f} | "
            f"{agreement_mark} |"
        )

    if len(report.verifications) > 20:
        md_lines.append(
            f"| ... | ... | ... | ... | ... | ... | *(–≤—Å–µ–≥–æ {len(report.verifications)})*"
        )

    md_lines.extend((
        "",
        "---",
        f"*–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*",
    ))

    return "\n".join(md_lines)


def load_test_collection_from_file(file_path: str) -> TestCollection:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—É—é —Å–±–æ—Ä–∫—É –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    with Path(file_path).open(encoding="utf-8") as f:
        data = json.load(f)

    topics = []
    for test in data.get("test_collection", {}).get("tests", []):
        questions = [Question(**q) for q in test.get("questions", [])]
        topics.append(
            Topic(
                topic_id=test.get("test_id", ""),
                topic_name=test.get("topic", ""),
                questions=questions,
            )
        )

    return TestCollection(
        creation_date=data.get("test_collection", {}).get("creation_date", ""),
        total_questions=data.get("test_collection", {}).get("total_questions", 0),
        topics_count=data.get("test_collection", {}).get("topics_count", 0),
        topics=topics,
    )


async def verify_answer(
    question: Question, language: str = "ru"
) -> tuple[PrimaryEvaluation, SecondaryEvaluation]:
    """–ü–æ–ª—É—á–∏—Ç—å –ø–µ—Ä–≤–∏—á–Ω—É—é –∏ –≤—Ç–æ—Ä–∏—á–Ω—É—é –æ—Ü–µ–Ω–∫—É –æ—Ç LLM"""

    try:
        primary_agent = load_agent("verification", language=language)
        primary_result = await primary_agent.ainvoke({
            "question": question.question_text,
            "expected_answer": question.expected_answer,
            "user_answer": question.expected_answer,
        })

        try:
            primary_eval_dict = json.loads(primary_result)
            primary_eval = PrimaryEvaluation(**primary_eval_dict)
        except (json.JSONDecodeError, ValueError):
            primary_eval = PrimaryEvaluation(
                score=75.0, is_correct=True, feedback="–û—Ç–≤–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —ç—Ç–∞–ª–æ–Ω—É"
            )

        secondary_agent = load_agent("verification-secondary", language=language)
        secondary_result = await secondary_agent.ainvoke({
            "primary_evaluation": json.dumps(primary_eval.model_dump(), ensure_ascii=False),
            "question": question.question_text,
            "user_answer": question.expected_answer,
        })

        try:
            secondary_eval_dict = json.loads(secondary_result)
            secondary_eval = SecondaryEvaluation(**secondary_eval_dict)
        except (json.JSONDecodeError, ValueError):
            secondary_eval = SecondaryEvaluation(
                agree_with_primary=True,
                final_score=75.0,
                final_feedback="–í—Ç–æ—Ä–∏—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∞ –æ—Ü–µ–Ω–∫—É",
                verification_notes="–ü—Ä–æ—à–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏",
            )

        return primary_eval, secondary_eval

    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–∞ {question.question_id}: {e}")
        return (
            PrimaryEvaluation(score=50.0, is_correct=False, feedback="–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏"),
            SecondaryEvaluation(
                agree_with_primary=False,
                final_score=50.0,
                final_feedback="–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Ç–æ—Ä–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ",
                verification_notes="–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞",
            ),
        )


async def process_verifications(
    test_collection: TestCollection, language: str = "ru"
) -> list[TestVerification]:
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    verifications = []
    total = test_collection.total_questions
    processed = 0

    for topic in test_collection.topics:
        for question in topic.questions:
            processed += 1
            print(
                f"  [{processed}/{total}] –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–ø—Ä–æ—Å–∞ {question.question_id} ({topic.topic_name})"
            )

            primary_eval, secondary_eval = await verify_answer(question, language)

            verification = TestVerification(
                question_id=question.question_id,
                topic=topic.topic_name,
                difficulty=question.difficulty,
                primary_evaluation=primary_eval,
                secondary_evaluation=secondary_eval,
                timestamp=datetime.now().isoformat(),
            )

            verifications.append(verification)

    return verifications


def calculate_metrics(verifications: list[TestVerification]) -> VerificationMetrics:
    """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏"""
    if not verifications:
        return VerificationMetrics(
            total_verifications=0,
            agreement_count=0,
            disagreement_count=0,
            agreement_rate=0.0,
            average_score_difference=0.0,
            correctness_match_rate=0.0,
        )

    total = len(verifications)
    agreements = sum(1 for v in verifications if v.secondary_evaluation.agree_with_primary)
    disagreements = total - agreements

    score_diffs = [
        abs(v.secondary_evaluation.final_score - v.primary_evaluation.score) for v in verifications
    ]
    avg_score_diff = sum(score_diffs) / len(score_diffs) if score_diffs else 0

    correctness_matches = sum(
        1
        for v in verifications
        if v.primary_evaluation.is_correct == (v.secondary_evaluation.final_score >= 70)
    )
    correctness_match_rate = (correctness_matches / total * 100) if total > 0 else 0

    return VerificationMetrics(
        total_verifications=total,
        agreement_count=agreements,
        disagreement_count=disagreements,
        agreement_rate=(agreements / total * 100) if total > 0 else 0,
        average_score_difference=avg_score_diff,
        correctness_match_rate=correctness_match_rate,
    )


def generate_report(verifications: list[TestVerification]) -> EffectivenessReport:
    """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç"""

    overall_metrics = calculate_metrics(verifications)

    # –ü–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    by_difficulty_dict = {}
    for difficulty in DifficultyLevel:
        diff_verifs = [v for v in verifications if v.difficulty == difficulty]
        if diff_verifs:
            by_difficulty_dict[difficulty] = calculate_metrics(diff_verifs)

    by_difficulty = [
        DifficultyMetrics(difficulty=diff, metrics=metrics)
        for diff, metrics in by_difficulty_dict.items()
    ]

    # –ü–æ —Ç–µ–º–∞–º
    by_topic_dict = {}
    for verification in verifications:
        if verification.topic not in by_topic_dict:
            by_topic_dict[verification.topic] = []
        by_topic_dict[verification.topic].append(verification)

    by_topic = [
        TopicMetrics(topic=topic, metrics=calculate_metrics(topic_verifs))
        for topic, topic_verifs in by_topic_dict.items()
    ]

    return EffectivenessReport(
        report_date=datetime.now().isoformat(),
        overall_metrics=overall_metrics,
        by_difficulty=by_difficulty,
        by_topic=by_topic,
        verifications=verifications,
    )


async def main(args: argparse.Namespace) -> None:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""

    print("üöÄ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—Ç–æ—Ä–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏\n")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑: {args.test_data}")
    try:
        test_collection = load_test_collection_from_file(args.test_data)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_collection.topics)} —Ç–µ–º")
        print(f"   –í—Å–µ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {test_collection.total_questions}\n")
    except FileNotFoundError:
        print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.test_data}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        sys.exit(1)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–π
    print("‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–π (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
    try:
        verifications = await process_verifications(test_collection, args.language)
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(verifications)} –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–π\n")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
        sys.exit(1)

    # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    print("üìä –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞...")
    report = generate_report(verifications)
    print("‚úÖ –û—Ç—á–µ—Ç –≥–æ—Ç–æ–≤\n")

    # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ markdown
    markdown_report = generate_markdown_report(report)
    print(markdown_report)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ markdown
    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with Path(output_path).open("w", encoding="utf-8") as f:
            f.write(markdown_report)

        print(f"\nüìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}")
        print(f"üìè –†–∞–∑–º–µ—Ä –æ—Ç—á–µ—Ç–∞: {len(markdown_report)} –±–∞–π—Ç")


def main_sync(args: argparse.Namespace) -> None:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—Ö–æ–¥ –¥–ª—è asyncio"""
    asyncio.run(main(args))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="–ò–∑–º–µ—Ä–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—Ç–æ—Ä–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ LLM"
    )

    parser.add_argument(
        "--test-data",
        type=str,
        default="test_data.json",
        help="–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏",
    )

    parser.add_argument(
        "--output",
        type=str,
        default="secondary_verification_report.md",
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞ (default: secondary_verification_report.md)",
    )

    parser.add_argument(
        "--language",
        type=str,
        default="ru",
        choices=["ru", "en"],
        help="–Ø–∑—ã–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (default: ru)",
    )

    args = parser.parse_args()
    main_sync(args)
